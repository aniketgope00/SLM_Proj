{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Website (Geeksforgeeks).        Topics</th>\n",
       "      <th>Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Machine Learning Tutorial</td>\n",
       "      <td>https://www.geeksforgeeks.org/machine-learning...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Python for Machine Learning</td>\n",
       "      <td>https://www.geeksforgeeks.org/python-for-machi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SQL for Machine Learning</td>\n",
       "      <td>https://www.geeksforgeeks.org/sql-for-machine-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Advantages and Disadvantages of Machine Learning</td>\n",
       "      <td>https://www.geeksforgeeks.org/what-is-machine-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Why ML is Important ?</td>\n",
       "      <td>https://www.geeksforgeeks.org/why-ml-is-import...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Website (Geeksforgeeks).        Topics  \\\n",
       "0                         Machine Learning Tutorial   \n",
       "1                       Python for Machine Learning   \n",
       "2                          SQL for Machine Learning   \n",
       "3  Advantages and Disadvantages of Machine Learning   \n",
       "4                             Why ML is Important ?   \n",
       "\n",
       "                                                Link  \n",
       "0  https://www.geeksforgeeks.org/machine-learning...  \n",
       "1  https://www.geeksforgeeks.org/python-for-machi...  \n",
       "2  https://www.geeksforgeeks.org/sql-for-machine-...  \n",
       "3  https://www.geeksforgeeks.org/what-is-machine-...  \n",
       "4  https://www.geeksforgeeks.org/why-ml-is-import...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "links_df = pd.read_csv(\"NLP_Project - Sheet2.csv\")\n",
    "links_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 151\n"
     ]
    }
   ],
   "source": [
    "n = links_df.shape[0]\n",
    "print(f\"Number of rows: {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Website (Geeksforgeeks).        Topics</th>\n",
       "      <th>Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ML | Understanding Data Processing</td>\n",
       "      <td>https://www.geeksforgeeks.org/ml-understanding...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Artificial Neural Networks and its Applications</td>\n",
       "      <td>https://www.geeksforgeeks.org/artificial-neura...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>CatBoost in Machine Learning</td>\n",
       "      <td>https://www.geeksforgeeks.org/catboost-ml/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Website (Geeksforgeeks).        Topics  \\\n",
       "14               ML | Understanding Data Processing   \n",
       "98  Artificial Neural Networks and its Applications   \n",
       "75                     CatBoost in Machine Learning   \n",
       "\n",
       "                                                 Link  \n",
       "14  https://www.geeksforgeeks.org/ml-understanding...  \n",
       "98  https://www.geeksforgeeks.org/artificial-neura...  \n",
       "75         https://www.geeksforgeeks.org/catboost-ml/  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_df = links_df.sample(n=3, random_state=1)\n",
    "small_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.geeksforgeeks.org/ml-understanding-data-processing/?ref=lbp'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get url\n",
    "small_df.iloc[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from firecrawl import FirecrawlApp\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "FIRECRAWL_API_KEY = os.getenv(\"FIRECRAWL_API_KEY\")\n",
    "app = FirecrawlApp(api_key=FIRECRAWL_API_KEY)\n",
    "\n",
    "def scrape_link(app, url : str)->str:\n",
    "    \"\"\"\n",
    "    Scrape the given URL using Firecrawl API.\n",
    "    \"\"\"\n",
    "    scrape_status = app.scrape_url(url, params={'format': ['markdown']})\n",
    "    if scrape_status[\"metadata\"][\"statusCode\"] == 200:\n",
    "        print(f\"Successfully scraped {url}\")\n",
    "        content = scrape_status[\"markdown\"]\n",
    "        return content\n",
    "    else:\n",
    "        print(f\"Error scraping {url}: {scrape_status['metadata']['statusCode']}\")\n",
    "        return None    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_scraped_data(content : str, idx: int)->None:\n",
    "    \"\"\"\n",
    "    Store the scraped data in a .md file.\n",
    "    \"\"\"\n",
    "    os.makedirs(\"scrapped_url_md\", exist_ok=True)\n",
    "    filename = f\"scrapped_url_md/file{idx}.md\"\n",
    "    #create file with the given filename\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "        print(f\"Stored scraped data in {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped https://www.geeksforgeeks.org/ml-understanding-data-processing/?ref=lbp\n",
      "Stored scraped data in scrapped_url_md/file0.md\n",
      "Successfully scraped https://www.geeksforgeeks.org/artificial-neural-networks-and-its-applications/\n",
      "Stored scraped data in scrapped_url_md/file1.md\n",
      "Successfully scraped https://www.geeksforgeeks.org/catboost-ml/\n",
      "Stored scraped data in scrapped_url_md/file2.md\n"
     ]
    }
   ],
   "source": [
    "from firecrawl import FirecrawlApp\n",
    "\n",
    "app = FirecrawlApp(api_key=\"fc-2a316f74c5d84fb9927b864137f6ac8e\")\n",
    "\n",
    "# Scrape a website:\n",
    "for i in range(3):\n",
    "    url = links_df.iloc[i, 1]\n",
    "    scrape_status = app.scrape_url(url, params={'formats': ['markdown']})\n",
    "    if scrape_status[\"metadata\"][\"statusCode\"] == 200:\n",
    "        print(f\"Successfully scraped {url}\")\n",
    "        content = scrape_status[\"markdown\"]\n",
    "        store_scraped_data(content, i)\n",
    "    else:\n",
    "        print(f\"Error scraping {url}: {scrape_status['metadata']['statusCode']}\")    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting marker-pdf\n",
      "  Using cached marker_pdf-1.6.2-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting Pillow<11.0.0,>=10.1.0 (from marker-pdf)\n",
      "  Downloading pillow-10.4.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (9.2 kB)\n",
      "Collecting anthropic<0.47.0,>=0.46.0 (from marker-pdf)\n",
      "  Using cached anthropic-0.46.0-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.7 in /Applications/anaconda3/lib/python3.10/site-packages (from marker-pdf) (8.1.7)\n",
      "Collecting filetype<2.0.0,>=1.2.0 (from marker-pdf)\n",
      "  Using cached filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting ftfy<7.0.0,>=6.1.1 (from marker-pdf)\n",
      "  Using cached ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting google-genai<2.0.0,>=1.0.0 (from marker-pdf)\n",
      "  Using cached google_genai-1.11.0-py3-none-any.whl.metadata (32 kB)\n",
      "Collecting markdown2<3.0.0,>=2.5.2 (from marker-pdf)\n",
      "  Using cached markdown2-2.5.3-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting markdownify<0.14.0,>=0.13.1 (from marker-pdf)\n",
      "  Using cached markdownify-0.13.1-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting openai<2.0.0,>=1.65.2 (from marker-pdf)\n",
      "  Using cached openai-1.75.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting pdftext<0.7.0,>=0.6.2 (from marker-pdf)\n",
      "  Using cached pdftext-0.6.2-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting pre-commit<5.0.0,>=4.2.0 (from marker-pdf)\n",
      "  Using cached pre_commit-4.2.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.4.2 in /Applications/anaconda3/lib/python3.10/site-packages (from marker-pdf) (2.10.3)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.0.3 in /Applications/anaconda3/lib/python3.10/site-packages (from marker-pdf) (2.6.1)\n",
      "Collecting python-dotenv<2.0.0,>=1.0.0 (from marker-pdf)\n",
      "  Using cached python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting rapidfuzz<4.0.0,>=3.8.1 (from marker-pdf)\n",
      "  Downloading rapidfuzz-3.13.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: regex<2025.0.0,>=2024.4.28 in /Applications/anaconda3/lib/python3.10/site-packages (from marker-pdf) (2024.11.6)\n",
      "Collecting scikit-learn<2.0.0,>=1.6.1 (from marker-pdf)\n",
      "  Using cached scikit_learn-1.6.1-cp310-cp310-macosx_12_0_arm64.whl.metadata (31 kB)\n",
      "Collecting surya-ocr<0.14.0,>=0.13.1 (from marker-pdf)\n",
      "  Using cached surya_ocr-0.13.1-py3-none-any.whl.metadata (31 kB)\n",
      "Collecting torch<3.0.0,>=2.5.1 (from marker-pdf)\n",
      "  Downloading torch-2.6.0-cp310-none-macosx_11_0_arm64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /Applications/anaconda3/lib/python3.10/site-packages (from marker-pdf) (4.66.5)\n",
      "Collecting transformers<5.0.0,>=4.45.2 (from marker-pdf)\n",
      "  Using cached transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Applications/anaconda3/lib/python3.10/site-packages (from anthropic<0.47.0,>=0.46.0->marker-pdf) (4.6.2)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Applications/anaconda3/lib/python3.10/site-packages (from anthropic<0.47.0,>=0.46.0->marker-pdf) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Applications/anaconda3/lib/python3.10/site-packages (from anthropic<0.47.0,>=0.46.0->marker-pdf) (0.27.0)\n",
      "Collecting jiter<1,>=0.4.0 (from anthropic<0.47.0,>=0.46.0->marker-pdf)\n",
      "  Downloading jiter-0.9.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: sniffio in /Applications/anaconda3/lib/python3.10/site-packages (from anthropic<0.47.0,>=0.46.0->marker-pdf) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in /Applications/anaconda3/lib/python3.10/site-packages (from anthropic<0.47.0,>=0.46.0->marker-pdf) (4.12.2)\n",
      "Requirement already satisfied: wcwidth in /Applications/anaconda3/lib/python3.10/site-packages (from ftfy<7.0.0,>=6.1.1->marker-pdf) (0.2.5)\n",
      "Collecting anyio<5,>=3.5.0 (from anthropic<0.47.0,>=0.46.0->marker-pdf)\n",
      "  Using cached anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting google-auth<3.0.0,>=2.14.1 (from google-genai<2.0.0,>=1.0.0->marker-pdf)\n",
      "  Using cached google_auth-2.39.0-py2.py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from anthropic<0.47.0,>=0.46.0->marker-pdf)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.28.1 in /Applications/anaconda3/lib/python3.10/site-packages (from google-genai<2.0.0,>=1.0.0->marker-pdf) (2.32.3)\n",
      "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /Applications/anaconda3/lib/python3.10/site-packages (from google-genai<2.0.0,>=1.0.0->marker-pdf) (15.0.1)\n",
      "Requirement already satisfied: beautifulsoup4<5,>=4.9 in /Applications/anaconda3/lib/python3.10/site-packages (from markdownify<0.14.0,>=0.13.1->marker-pdf) (4.12.3)\n",
      "Requirement already satisfied: six<2,>=1.15 in /Applications/anaconda3/lib/python3.10/site-packages (from markdownify<0.14.0,>=0.13.1->marker-pdf) (1.16.0)\n",
      "Collecting click<9.0.0,>=8.1.7 (from marker-pdf)\n",
      "  Using cached click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting pypdfium2==4.30.0 (from pdftext<0.7.0,>=0.6.2->marker-pdf)\n",
      "  Using cached pypdfium2-4.30.0-py3-none-macosx_11_0_arm64.whl.metadata (48 kB)\n",
      "Collecting cfgv>=2.0.0 (from pre-commit<5.0.0,>=4.2.0->marker-pdf)\n",
      "  Using cached cfgv-3.4.0-py2.py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting identify>=1.0.0 (from pre-commit<5.0.0,>=4.2.0->marker-pdf)\n",
      "  Using cached identify-2.6.9-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting nodeenv>=0.11.1 (from pre-commit<5.0.0,>=4.2.0->marker-pdf)\n",
      "  Using cached nodeenv-1.9.1-py2.py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Applications/anaconda3/lib/python3.10/site-packages (from pre-commit<5.0.0,>=4.2.0->marker-pdf) (6.0.2)\n",
      "Collecting virtualenv>=20.10.0 (from pre-commit<5.0.0,>=4.2.0->marker-pdf)\n",
      "  Using cached virtualenv-20.30.0-py3-none-any.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Applications/anaconda3/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.4.2->marker-pdf) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /Applications/anaconda3/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.4.2->marker-pdf) (2.27.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /Applications/anaconda3/lib/python3.10/site-packages (from scikit-learn<2.0.0,>=1.6.1->marker-pdf) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Applications/anaconda3/lib/python3.10/site-packages (from scikit-learn<2.0.0,>=1.6.1->marker-pdf) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Applications/anaconda3/lib/python3.10/site-packages (from scikit-learn<2.0.0,>=1.6.1->marker-pdf) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Applications/anaconda3/lib/python3.10/site-packages (from scikit-learn<2.0.0,>=1.6.1->marker-pdf) (3.5.0)\n",
      "Collecting opencv-python-headless<5.0.0.0,>=4.11.0.86 (from surya-ocr<0.14.0,>=0.13.1->marker-pdf)\n",
      "  Using cached opencv_python_headless-4.11.0.86-cp37-abi3-macosx_13_0_arm64.whl.metadata (20 kB)\n",
      "Collecting platformdirs<5.0.0,>=4.3.6 (from surya-ocr<0.14.0,>=0.13.1->marker-pdf)\n",
      "  Using cached platformdirs-4.3.7-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: filelock in /Applications/anaconda3/lib/python3.10/site-packages (from torch<3.0.0,>=2.5.1->marker-pdf) (3.13.1)\n",
      "Requirement already satisfied: networkx in /Applications/anaconda3/lib/python3.10/site-packages (from torch<3.0.0,>=2.5.1->marker-pdf) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Applications/anaconda3/lib/python3.10/site-packages (from torch<3.0.0,>=2.5.1->marker-pdf) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Applications/anaconda3/lib/python3.10/site-packages (from torch<3.0.0,>=2.5.1->marker-pdf) (2024.12.0)\n",
      "Collecting sympy==1.13.1 (from torch<3.0.0,>=2.5.1->marker-pdf)\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Applications/anaconda3/lib/python3.10/site-packages (from sympy==1.13.1->torch<3.0.0,>=2.5.1->marker-pdf) (1.3.0)\n",
      "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers<5.0.0,>=4.45.2->marker-pdf)\n",
      "  Using cached huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /Applications/anaconda3/lib/python3.10/site-packages (from transformers<5.0.0,>=4.45.2->marker-pdf) (24.2)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers<5.0.0,>=4.45.2->marker-pdf)\n",
      "  Using cached tokenizers-0.21.1-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers<5.0.0,>=4.45.2->marker-pdf)\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Applications/anaconda3/lib/python3.10/site-packages (from anyio<5,>=3.5.0->anthropic<0.47.0,>=0.46.0->marker-pdf) (1.2.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Applications/anaconda3/lib/python3.10/site-packages (from anyio<5,>=3.5.0->anthropic<0.47.0,>=0.46.0->marker-pdf) (3.7)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Applications/anaconda3/lib/python3.10/site-packages (from beautifulsoup4<5,>=4.9->markdownify<0.14.0,>=0.13.1->marker-pdf) (2.5)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Applications/anaconda3/lib/python3.10/site-packages (from google-auth<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.0.0->marker-pdf) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Applications/anaconda3/lib/python3.10/site-packages (from google-auth<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.0.0->marker-pdf) (0.2.8)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.0.0->marker-pdf)\n",
      "  Using cached rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: certifi in /Applications/anaconda3/lib/python3.10/site-packages (from httpx<1,>=0.23.0->anthropic<0.47.0,>=0.46.0->marker-pdf) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in /Applications/anaconda3/lib/python3.10/site-packages (from httpx<1,>=0.23.0->anthropic<0.47.0,>=0.46.0->marker-pdf) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Applications/anaconda3/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->anthropic<0.47.0,>=0.46.0->marker-pdf) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Applications/anaconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.28.1->google-genai<2.0.0,>=1.0.0->marker-pdf) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Applications/anaconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.28.1->google-genai<2.0.0,>=1.0.0->marker-pdf) (2.3.0)\n",
      "Collecting distlib<1,>=0.3.7 (from virtualenv>=20.10.0->pre-commit<5.0.0,>=4.2.0->marker-pdf)\n",
      "  Using cached distlib-0.3.9-py2.py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Applications/anaconda3/lib/python3.10/site-packages (from jinja2->torch<3.0.0,>=2.5.1->marker-pdf) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Applications/anaconda3/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.0.0->marker-pdf) (0.4.8)\n",
      "Using cached marker_pdf-1.6.2-py3-none-any.whl (165 kB)\n",
      "Using cached anthropic-0.46.0-py3-none-any.whl (223 kB)\n",
      "Using cached filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Using cached ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
      "Using cached google_genai-1.11.0-py3-none-any.whl (159 kB)\n",
      "Using cached markdown2-2.5.3-py3-none-any.whl (48 kB)\n",
      "Using cached markdownify-0.13.1-py3-none-any.whl (10 kB)\n",
      "Using cached openai-1.75.0-py3-none-any.whl (646 kB)\n",
      "Using cached pdftext-0.6.2-py3-none-any.whl (23 kB)\n",
      "Using cached pypdfium2-4.30.0-py3-none-macosx_11_0_arm64.whl (2.7 MB)\n",
      "Using cached click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Downloading pillow-10.4.0-cp310-cp310-macosx_11_0_arm64.whl (3.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hUsing cached pre_commit-4.2.0-py2.py3-none-any.whl (220 kB)\n",
      "Using cached python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Downloading rapidfuzz-3.13.0-cp310-cp310-macosx_11_0_arm64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached scikit_learn-1.6.1-cp310-cp310-macosx_12_0_arm64.whl (11.1 MB)\n",
      "Using cached surya_ocr-0.13.1-py3-none-any.whl (154 kB)\n",
      "Downloading torch-2.6.0-cp310-none-macosx_11_0_arm64.whl (66.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.5/66.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Using cached transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "Using cached anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "Using cached cfgv-3.4.0-py2.py3-none-any.whl (7.2 kB)\n",
      "Using cached google_auth-2.39.0-py2.py3-none-any.whl (212 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
      "Using cached identify-2.6.9-py2.py3-none-any.whl (99 kB)\n",
      "Downloading jiter-0.9.0-cp310-cp310-macosx_11_0_arm64.whl (321 kB)\n",
      "Using cached nodeenv-1.9.1-py2.py3-none-any.whl (22 kB)\n",
      "Using cached opencv_python_headless-4.11.0.86-cp37-abi3-macosx_13_0_arm64.whl (37.3 MB)\n",
      "Using cached platformdirs-4.3.7-py3-none-any.whl (18 kB)\n",
      "Using cached safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl (418 kB)\n",
      "Using cached tokenizers-0.21.1-cp39-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
      "Using cached virtualenv-20.30.0-py3-none-any.whl (4.3 MB)\n",
      "Using cached distlib-0.3.9-py2.py3-none-any.whl (468 kB)\n",
      "Using cached rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Installing collected packages: filetype, distlib, sympy, safetensors, rsa, rapidfuzz, python-dotenv, pypdfium2, platformdirs, Pillow, opencv-python-headless, nodeenv, markdown2, jiter, identify, ftfy, click, cfgv, anyio, virtualenv, torch, scikit-learn, markdownify, huggingface-hub, httpx, google-auth, tokenizers, pre-commit, openai, google-genai, anthropic, transformers, pdftext, surya-ocr, marker-pdf\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.3\n",
      "    Uninstalling sympy-1.13.3:\n",
      "      Successfully uninstalled sympy-1.13.3\n",
      "  Attempting uninstall: python-dotenv\n",
      "    Found existing installation: python-dotenv 0.21.0\n",
      "    Uninstalling python-dotenv-0.21.0:\n",
      "      Successfully uninstalled python-dotenv-0.21.0\n",
      "  Attempting uninstall: platformdirs\n",
      "    Found existing installation: platformdirs 3.10.0\n",
      "    Uninstalling platformdirs-3.10.0:\n",
      "      Successfully uninstalled platformdirs-3.10.0\n",
      "  Attempting uninstall: Pillow\n",
      "    Found existing installation: pillow 11.0.0\n",
      "    Uninstalling pillow-11.0.0:\n",
      "      Successfully uninstalled pillow-11.0.0\n",
      "  Attempting uninstall: click\n",
      "    Found existing installation: click 8.1.7\n",
      "    Uninstalling click-8.1.7:\n",
      "      Successfully uninstalled click-8.1.7\n",
      "  Attempting uninstall: anyio\n",
      "    Found existing installation: anyio 4.6.2\n",
      "    Uninstalling anyio-4.6.2:\n",
      "      Successfully uninstalled anyio-4.6.2\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.5.2\n",
      "    Uninstalling scikit-learn-1.5.2:\n",
      "      Successfully uninstalled scikit-learn-1.5.2\n",
      "  Attempting uninstall: httpx\n",
      "    Found existing installation: httpx 0.27.0\n",
      "    Uninstalling httpx-0.27.0:\n",
      "      Successfully uninstalled httpx-0.27.0\n",
      "Successfully installed Pillow-10.4.0 anthropic-0.46.0 anyio-4.9.0 cfgv-3.4.0 click-8.1.8 distlib-0.3.9 filetype-1.2.0 ftfy-6.3.1 google-auth-2.39.0 google-genai-1.11.0 httpx-0.28.1 huggingface-hub-0.30.2 identify-2.6.9 jiter-0.9.0 markdown2-2.5.3 markdownify-0.13.1 marker-pdf-1.6.2 nodeenv-1.9.1 openai-1.75.0 opencv-python-headless-4.11.0.86 pdftext-0.6.2 platformdirs-4.3.7 pre-commit-4.2.0 pypdfium2-4.30.0 python-dotenv-1.1.0 rapidfuzz-3.13.0 rsa-4.9.1 safetensors-0.5.3 scikit-learn-1.6.1 surya-ocr-0.13.1 sympy-1.13.1 tokenizers-0.21.1 torch-2.6.0 transformers-4.51.3 virtualenv-20.30.0\n"
     ]
    }
   ],
   "source": [
    "!pip install marker-pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded layout model s3://layout/2025_02_18 on device mps with dtype torch.float16\n",
      "Loaded texify model s3://texify/2025_02_18 on device mps with dtype torch.float16\n",
      "Loaded recognition model s3://text_recognition/2025_02_18 on device mps with dtype torch.float16\n",
      "Loaded table recognition model s3://table_recognition/2025_02_18 on device mps with dtype torch.float16\n",
      "Loaded detection model s3://text_detection/2025_02_28 on device mps with dtype torch.float16\n",
      "Loaded detection model s3://inline_math_detection/2025_02_24 on device mps with dtype torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Recognizing layout: 100%|██████████| 2/2 [00:28<00:00, 14.28s/it]\n",
      "Running OCR Error Detection: 100%|██████████| 3/3 [00:06<00:00,  2.00s/it]\n",
      "Detecting bboxes: 0it [00:00, ?it/s]\n",
      "Texify inference: 100%|██████████| 1/1 [00:14<00:00, 14.04s/it]\n",
      "Detecting bboxes: 0it [00:00, ?it/s]\n",
      "Recognizing tables: 100%|██████████| 1/1 [00:36<00:00, 36.22s/it]\n",
      "/Applications/anaconda3/lib/python3.10/site-packages/marker/renderers/html.py:47: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(document_output.html, 'html.parser')\n",
      "/Applications/anaconda3/lib/python3.10/site-packages/marker/renderers/html.py:79: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  ref.replace_with(BeautifulSoup(f\"{content}\", 'html.parser'))\n"
     ]
    }
   ],
   "source": [
    "PATH = \"test_pdfs/AttentionIsAllYouNeed.pdf\"\n",
    "from marker.converters.pdf import PdfConverter\n",
    "from marker.models import create_model_dict\n",
    "from marker.output import text_from_rendered\n",
    "\n",
    "converter = PdfConverter(artifact_dict=create_model_dict(),)\n",
    "rendered = converter(PATH)  # Fix: Use '=' instead of '-'\n",
    "text, _, images = text_from_rendered(rendered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Attention Is All You Need\n",
      "\n",
      "Ashish Vaswani<sup>∗</sup> Google Brain avaswani@google.com\n",
      "\n",
      "Noam Shazeer<sup>∗</sup> Google Brain noam@google.com\n",
      "\n",
      "Niki Parmar<sup>∗</sup> Google Research nikip@google.com\n",
      "\n",
      "Jakob Uszkoreit<sup>∗</sup> Google Research usz@google.com\n",
      "\n",
      "Llion Jones<sup>∗</sup> Google Research llion@google.com\n",
      "\n",
      "Aidan N. Gomez∗ † University of Toronto aidan@cs.toronto.edu\n",
      "\n",
      "Łukasz Kaiser<sup>∗</sup> Google Brain lukaszkaiser@google.com\n",
      "\n",
      "Illia Polosukhin∗ ‡ illia.polosukhin@gmail.com\n",
      "\n",
      "### Abstract\n",
      "\n",
      "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.\n",
      "\n",
      "### 1 Introduction\n",
      "\n",
      "Recurrent neural networks, long short-term memory [12] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [29, 2, 5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [31, 21, 13].\n",
      "\n",
      "‡Work performed while at Google Research.\n",
      "\n",
      "31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\n",
      "\n",
      "<sup>∗</sup>Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.\n",
      "\n",
      "<sup>†</sup>Work performed while at Google Brain.\n",
      "\n",
      "Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states ht, as a function of the previous hidden state ht−<sup>1</sup> and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved significant improvements in computational efficiency through factorization tricks [18] and conditional computation [26], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains.\n",
      "\n",
      "Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms are used in conjunction with a recurrent network.\n",
      "\n",
      "In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
      "\n",
      "## 2 Background\n",
      "\n",
      "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions [11]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2.\n",
      "\n",
      "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 22, 23, 19].\n",
      "\n",
      "End-to-end memory networks are based on a recurrent attention mechanism instead of sequencealigned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [28].\n",
      "\n",
      "To the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequencealigned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [14, 15] and [8].\n",
      "\n",
      "### 3 Model Architecture\n",
      "\n",
      "Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29]. Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence of continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output sequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive [9], consuming the previously generated symbols as additional input when generating the next.\n",
      "\n",
      "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.\n",
      "\n",
      "### 3.1 Encoder and Decoder Stacks\n",
      "\n",
      "Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\n",
      "\n",
      "![](_page_2_Figure_0.jpeg)\n",
      "\n",
      "Figure 1: The Transformer - model architecture.\n",
      "\n",
      "wise fully connected feed-forward network. We employ a residual connection [10] around each of the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512.\n",
      "\n",
      "Decoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i.\n",
      "\n",
      "### 3.2 Attention\n",
      "\n",
      "An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\n",
      "\n",
      "### 3.2.1 Scaled Dot-Product Attention\n",
      "\n",
      "We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\n",
      "\n",
      "![](_page_3_Figure_0.jpeg)\n",
      "\n",
      "Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel.\n",
      "\n",
      "query with all keys, divide each by <sup>√</sup> dk, and apply a softmax function to obtain the weights on the values.\n",
      "\n",
      "In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q. The keys and values are also packed together into matrices K and V . We compute the matrix of outputs as:\n",
      "\n",
      "$$\\text{Attention}(Q, K, V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d\\_k}})V \\tag{l}$$\n",
      "\n",
      "The two most commonly used attention functions are additive attention [2], and dot-product (multiplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor of <sup>√</sup> 1 d<sup>k</sup> . Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.\n",
      "\n",
      "While for small values of d<sup>k</sup> the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of d<sup>k</sup> [3]. We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients 4 . To counteract this effect, we scale the dot products by <sup>√</sup> 1 d<sup>k</sup> .\n",
      "\n",
      "#### 3.2.2 Multi-Head Attention\n",
      "\n",
      "Instead of performing a single attention function with dmodel-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to dk, d<sup>k</sup> and d<sup>v</sup> dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional output values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2.\n",
      "\n",
      "Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.\n",
      "\n",
      "<sup>4</sup>To illustrate why the dot products get large, assume that the components of q and k are independent random variables with mean 0 and variance 1. Then their dot product, q · k = P<sup>d</sup><sup>k</sup> <sup>i</sup>=1 qiki, has mean 0 and variance dk.\n",
      "\n",
      "MultiHead(Q, K, V ) = Concat(head1, ..., headh)W<sup>O</sup> where head<sup>i</sup> = Attention(QW<sup>Q</sup> i , KW <sup>K</sup> i , V W<sup>V</sup> i )\n",
      "\n",
      "Where the projections are parameter matrices W Q <sup>i</sup> <sup>∈</sup> <sup>R</sup> <sup>d</sup>model×d<sup>k</sup> , W <sup>K</sup> <sup>i</sup> ∈ R <sup>d</sup>model×d<sup>k</sup> , W<sup>V</sup> <sup>i</sup> ∈ R dmodel×d<sup>v</sup> and W<sup>O</sup> ∈ R hdv×dmodel .\n",
      "\n",
      "In this work we employ h = 8 parallel attention layers, or heads. For each of these we use d<sup>k</sup> = d<sup>v</sup> = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.\n",
      "\n",
      "#### 3.2.3 Applications of Attention in our Model\n",
      "\n",
      "The Transformer uses multi-head attention in three different ways:\n",
      "\n",
      "- In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [31, 2, 8].\n",
      "- The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.\n",
      "- Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to −∞) all values in the input of the softmax which correspond to illegal connections. See Figure 2.\n",
      "\n",
      "### 3.3 Position-wise Feed-Forward Networks\n",
      "\n",
      "In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between.\n",
      "\n",
      "$$\\text{FFN}(x) = \\max(0, xW\\_1 + b\\_1)W\\_2 + b\\_2 \\tag{2}$$\n",
      "\n",
      "While the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality df f = 2048.\n",
      "\n",
      "#### 3.4 Embeddings and Softmax\n",
      "\n",
      "Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [24]. In the embedding layers, we multiply those weights by <sup>√</sup> dmodel.\n",
      "\n",
      "#### 3.5 Positional Encoding\n",
      "\n",
      "Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\n",
      "\n",
      "Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types. n is the sequence length, d is the representation dimension, k is the kernel size of convolutions and r the size of the neighborhood in restricted self-attention.\n",
      "\n",
      "| Layer Type                  | Complexity per Layer  | Sequential<br>Operations | Maximum Path Length |\n",
      "|-----------------------------|-----------------------|--------------------------|---------------------|\n",
      "| Self-Attention              | 2<br>O(n<br>· d)      | O(1)                     | O(1)                |\n",
      "| Recurrent                   | 2<br>O(n · d<br>)     | O(n)                     | O(n)                |\n",
      "| Convolutional               | 2<br>O(k · n · d<br>) | O(1)                     | O(logk(n))          |\n",
      "| Self-Attention (restricted) | O(r · n · d)          | O(1)                     | O(n/r)              |\n",
      "\n",
      "bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed [8].\n",
      "\n",
      "In this work, we use sine and cosine functions of different frequencies:\n",
      "\n",
      "$$PE\\_{(pos,2i)} = \\sin(pos/10000^{2i/d\\_{\\text{model}}})$$\n",
      "\n",
      "$$PE\\_{(pos,2i+1)} = \\cos(pos/10000^{2i/d\\_{\\text{model}}})$$\n",
      "\n",
      "where pos is the position and i is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k, P Epos+<sup>k</sup> can be represented as a linear function of P Epos.\n",
      "\n",
      "We also experimented with using learned positional embeddings [8] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.\n",
      "\n",
      "### 4 Why Self-Attention\n",
      "\n",
      "In this section we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations (x1, ..., xn) to another sequence of equal length (z1, ..., zn), with x<sup>i</sup> , z<sup>i</sup> ∈ R d , such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata.\n",
      "\n",
      "One is the total computational complexity per layer. Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required.\n",
      "\n",
      "The third is the path length between long-range dependencies in the network. Learning long-range dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network. The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies [11]. Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types.\n",
      "\n",
      "As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [31] and byte-pair [25] representations. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r in the input sequence centered around the respective output position. This would increase the maximum path length to O(n/r). We plan to investigate this approach further in future work.\n",
      "\n",
      "A single convolutional layer with kernel width k < n does not connect all pairs of input and output positions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels, or O(logk(n)) in the case of dilated convolutions [15], increasing the length of the longest paths between any two positions in the network. Convolutional layers are generally more expensive than recurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity considerably, to O(k · n · d + n · d 2 ). Even with k = n, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model.\n",
      "\n",
      "As side benefit, self-attention could yield more interpretable models. We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences.\n",
      "\n",
      "### 5 Training\n",
      "\n",
      "This section describes the training regime for our models.\n",
      "\n",
      "#### 5.1 Training Data and Batching\n",
      "\n",
      "We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared sourcetarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [31]. Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.\n",
      "\n",
      "#### 5.2 Hardware and Schedule\n",
      "\n",
      "We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days).\n",
      "\n",
      "#### 5.3 Optimizer\n",
      "\n",
      "We used the Adam optimizer [17] with β<sup>1</sup> = 0.9, β<sup>2</sup> = 0.98 and = 10<sup>−</sup><sup>9</sup> . We varied the learning rate over the course of training, according to the formula:\n",
      "\n",
      "$$rate = d\\_{\\text{model}}^{-0.5} \\cdot \\min(step\\\\_num^{-0.5}, step\\\\_num \\cdot warmup\\\\_steps^{-1.5}) \\tag{3}$$\n",
      "\n",
      "This corresponds to increasing the learning rate linearly for the first warmup\\_steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup\\_steps = 4000.\n",
      "\n",
      "### 5.4 Regularization\n",
      "\n",
      "We employ three types of regularization during training:\n",
      "\n",
      "Residual Dropout We apply dropout [27] to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of Pdrop = 0.1.\n",
      "\n",
      "|                                 | BLEU           |       | Training Cost (FLOPs) |            |  |  |\n",
      "|---------------------------------|----------------|-------|-----------------------|------------|--|--|\n",
      "| Model                           | EN-DE<br>EN-FR |       | EN-DE                 | EN-FR      |  |  |\n",
      "| ByteNet [15]                    | 23.75          |       |                       |            |  |  |\n",
      "| Deep-Att + PosUnk [32]          |                | 39.2  |                       | 1.0 · 1020 |  |  |\n",
      "| GNMT + RL [31]                  | 24.6           | 39.92 | 2.3 · 1019            | 1.4 · 1020 |  |  |\n",
      "| ConvS2S [8]                     | 25.16          | 40.46 | 9.6 · 1018            | 1.5 · 1020 |  |  |\n",
      "| MoE [26]                        | 26.03          | 40.56 | 2.0 · 1019            | 1.2 · 1020 |  |  |\n",
      "| Deep-Att + PosUnk Ensemble [32] |                | 40.4  |                       | 8.0 · 1020 |  |  |\n",
      "| GNMT + RL Ensemble [31]         | 26.30          | 41.16 | 1.8 · 1020            | 1.1 · 1021 |  |  |\n",
      "| ConvS2S Ensemble [8]            | 26.36          | 41.29 | 7.7 · 1019            | 1.2 · 1021 |  |  |\n",
      "| Transformer (base model)        | 27.3           | 38.1  |                       | 3.3 · 1018 |  |  |\n",
      "| Transformer (big)               | 28.4           | 41.0  |                       | 2.3 · 1019 |  |  |\n",
      "\n",
      "Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\n",
      "\n",
      "Label Smoothing During training, we employed label smoothing of value ls = 0.1 [30]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\n",
      "\n",
      "### 6 Results\n",
      "\n",
      "### 6.1 Machine Translation\n",
      "\n",
      "On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0 BLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is listed in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models.\n",
      "\n",
      "On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0, outperforming all of the previously published single models, at less than 1/4 the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate Pdrop = 0.1, instead of 0.3.\n",
      "\n",
      "For the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We used beam search with a beam size of 4 and length penalty α = 0.6 [31]. These hyperparameters were chosen after experimentation on the development set. We set the maximum output length during inference to input length + 50, but terminate early when possible [31].\n",
      "\n",
      "Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature. We estimate the number of floating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision floating-point capacity of each GPU 5 .\n",
      "\n",
      "### 6.2 Model Variations\n",
      "\n",
      "To evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the development set, newstest2013. We used beam search as described in the previous section, but no checkpoint averaging. We present these results in Table 3.\n",
      "\n",
      "In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\n",
      "\n",
      "<sup>5</sup>We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\n",
      "\n",
      "Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base model. All metrics are on the English-to-German translation development set, newstest2013. Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities.\n",
      "\n",
      "|      | N                                         | dmodel | dff  | h  | dk  | dv  | Pdrop | ls   | train | PPL   | BLEU  | params |\n",
      "|------|-------------------------------------------|--------|------|----|-----|-----|-------|------|-------|-------|-------|--------|\n",
      "|      |                                           |        |      |    |     |     |       |      | steps | (dev) | (dev) | ×106   |\n",
      "| base | 6                                         | 512    | 2048 | 8  | 64  | 64  | 0.1   | 0.1  | 100K  | 4.92  | 25.8  | 65     |\n",
      "|      |                                           |        |      | 1  | 512 | 512 |       |      |       | 5.29  | 24.9  |        |\n",
      "|      |                                           |        |      | 4  | 128 | 128 |       |      |       | 5.00  | 25.5  |        |\n",
      "| (A)  |                                           |        |      | 16 | 32  | 32  |       |      |       | 4.91  | 25.8  |        |\n",
      "|      |                                           |        |      | 32 | 16  | 16  |       |      |       | 5.01  | 25.4  |        |\n",
      "| (B)  |                                           |        |      |    | 16  |     |       |      |       | 5.16  | 25.1  | 58     |\n",
      "|      |                                           |        |      |    | 32  |     |       |      |       | 5.01  | 25.4  | 60     |\n",
      "| (C)  | 2                                         |        |      |    |     |     |       |      |       | 6.11  | 23.7  | 36     |\n",
      "|      | 4                                         |        |      |    |     |     |       |      |       | 5.19  | 25.3  | 50     |\n",
      "|      | 8                                         |        |      |    |     |     |       |      |       | 4.88  | 25.5  | 80     |\n",
      "|      |                                           | 256    |      |    | 32  | 32  |       |      |       | 5.75  | 24.5  | 28     |\n",
      "|      |                                           | 1024   |      |    | 128 | 128 |       |      |       | 4.66  | 26.0  | 168    |\n",
      "|      |                                           |        | 1024 |    |     |     |       |      |       | 5.12  | 25.4  | 53     |\n",
      "|      |                                           |        | 4096 |    |     |     |       |      |       | 4.75  | 26.2  | 90     |\n",
      "| (D)  |                                           |        |      |    |     |     | 0.0   |      |       | 5.77  | 24.6  |        |\n",
      "|      |                                           |        |      |    |     |     | 0.2   |      |       | 4.95  | 25.5  |        |\n",
      "|      |                                           |        |      |    |     |     |       | 0.0  |       | 4.67  | 25.3  |        |\n",
      "|      |                                           |        |      |    |     |     |       | 0.2  |       | 5.47  | 25.7  |        |\n",
      "| (E)  | positional embedding instead of sinusoids |        |      |    |     |     |       | 4.92 | 25.7  |       |       |        |\n",
      "| big  | 6                                         | 1024   | 4096 | 16 |     |     | 0.3   |      | 300K  | 4.33  | 26.4  | 213    |\n",
      "\n",
      "In Table 3 rows (B), we observe that reducing the attention key size d<sup>k</sup> hurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [8], and observe nearly identical results to the base model.\n",
      "\n",
      "# 7 Conclusion\n",
      "\n",
      "In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.\n",
      "\n",
      "For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles.\n",
      "\n",
      "We are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours.\n",
      "\n",
      "The code we used to train and evaluate our models is available at [https://github.com/](https://github.com/tensorflow/tensor2tensor) [tensorflow/tensor2tensor](https://github.com/tensorflow/tensor2tensor).\n",
      "\n",
      "Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments, corrections and inspiration.\n",
      "\n",
      "### References\n",
      "\n",
      "- [1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. *arXiv preprint arXiv:1607.06450*, 2016.\n",
      "- [2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. *CoRR*, abs/1409.0473, 2014.\n",
      "- [3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural machine translation architectures. *CoRR*, abs/1703.03906, 2017.\n",
      "- [4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine reading. *arXiv preprint arXiv:1601.06733*, 2016.\n",
      "- [5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. *CoRR*, abs/1406.1078, 2014.\n",
      "- [6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. *arXiv preprint arXiv:1610.02357*, 2016.\n",
      "- [7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. *CoRR*, abs/1412.3555, 2014.\n",
      "- [8] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolutional sequence to sequence learning. *arXiv preprint arXiv:1705.03122v2*, 2017.\n",
      "- [9] Alex Graves. Generating sequences with recurrent neural networks. *arXiv preprint arXiv:1308.0850*, 2013.\n",
      "- [10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*, pages 770–778, 2016.\n",
      "- [11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in recurrent nets: the difficulty of learning long-term dependencies, 2001.\n",
      "- [12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. *Neural computation*, 9(8):1735–1780, 1997.\n",
      "- [13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of language modeling. *arXiv preprint arXiv:1602.02410*, 2016.\n",
      "- [14] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In *International Conference on Learning Representations (ICLR)*, 2016.\n",
      "- [15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Koray Kavukcuoglu. Neural machine translation in linear time. *arXiv preprint arXiv:1610.10099v2*, 2017.\n",
      "- [16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks. In *International Conference on Learning Representations*, 2017.\n",
      "- [17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In *ICLR*, 2015.\n",
      "- [18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. *arXiv preprint arXiv:1703.10722*, 2017.\n",
      "- [19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. *arXiv preprint arXiv:1703.03130*, 2017.\n",
      "- [20] Samy Bengio Łukasz Kaiser. Can active memory replace attention? In *Advances in Neural Information Processing Systems, (NIPS)*, 2016.\n",
      "- [21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attentionbased neural machine translation. *arXiv preprint arXiv:1508.04025*, 2015.\n",
      "- [22] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention model. In *Empirical Methods in Natural Language Processing*, 2016.\n",
      "- [23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive summarization. *arXiv preprint arXiv:1705.04304*, 2017.\n",
      "- [24] Ofir Press and Lior Wolf. Using the output embedding to improve language models. *arXiv preprint arXiv:1608.05859*, 2016.\n",
      "- [25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. *arXiv preprint arXiv:1508.07909*, 2015.\n",
      "- [26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. *arXiv preprint arXiv:1701.06538*, 2017.\n",
      "- [27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. *Journal of Machine Learning Research*, 15(1):1929–1958, 2014.\n",
      "- [28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, *Advances in Neural Information Processing Systems 28*, pages 2440–2448. Curran Associates, Inc., 2015.\n",
      "- [29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural networks. In *Advances in Neural Information Processing Systems*, pages 3104–3112, 2014.\n",
      "- [30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. *CoRR*, abs/1512.00567, 2015.\n",
      "- [31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google's neural machine translation system: Bridging the gap between human and machine translation. *arXiv preprint arXiv:1609.08144*, 2016.\n",
      "- [32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with fast-forward connections for neural machine translation. *CoRR*, abs/1606.04199, 2016.\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_page_2_Figure_0.jpeg': <PIL.Image.Image image mode=RGB size=592x868 at 0x392D2C880>, '_page_3_Figure_0.jpeg': <PIL.Image.Image image mode=RGB size=849x449 at 0x392D2F280>}\n"
     ]
    }
   ],
   "source": [
    "print(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded layout model s3://layout/2025_02_18 on device mps with dtype torch.float16\n",
      "Loaded texify model s3://texify/2025_02_18 on device mps with dtype torch.float16\n",
      "Loaded recognition model s3://text_recognition/2025_02_18 on device mps with dtype torch.float16\n",
      "Loaded table recognition model s3://table_recognition/2025_02_18 on device mps with dtype torch.float16\n",
      "Loaded detection model s3://text_detection/2025_02_28 on device mps with dtype torch.float16\n",
      "Loaded detection model s3://inline_math_detection/2025_02_24 on device mps with dtype torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Recognizing layout: 100%|██████████| 127/127 [2:06:21<00:00, 59.70s/it] \n",
      "Running OCR Error Detection: 100%|██████████| 190/190 [1:15:43<00:00, 23.91s/it]\n",
      "Detecting bboxes: 0it [00:00, ?it/s]\n",
      "Texify inference: 100%|██████████| 358/358 [3:45:23<00:00, 37.78s/it]  \n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (769 > 768). Running this sequence through the model will result in indexing errors\n",
      "Detecting bboxes: 0it [00:00, ?it/s]\n",
      "Recognizing tables: 100%|██████████| 3/3 [04:12<00:00, 84.12s/it]\n",
      "/Applications/anaconda3/lib/python3.10/site-packages/marker/renderers/html.py:47: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  soup = BeautifulSoup(document_output.html, 'html.parser')\n",
      "/Applications/anaconda3/lib/python3.10/site-packages/marker/renderers/html.py:79: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ref.replace_with(BeautifulSoup(f\"{content}\", 'html.parser'))\n",
      "/Applications/anaconda3/lib/python3.10/site-packages/marker/renderers/html.py:47: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(document_output.html, 'html.parser')\n",
      "/Applications/anaconda3/lib/python3.10/site-packages/marker/renderers/html.py:79: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  ref.replace_with(BeautifulSoup(f\"{content}\", 'html.parser'))\n"
     ]
    }
   ],
   "source": [
    "PATH = \"test_pdfs/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf\"\n",
    "from marker.converters.pdf import PdfConverter\n",
    "from marker.models import create_model_dict\n",
    "from marker.output import text_from_rendered\n",
    "\n",
    "converter = PdfConverter(artifact_dict=create_model_dict(),)\n",
    "rendered = converter(PATH)  # Fix: Use '=' instead of '-'\n",
    "text, _, images = text_from_rendered(rendered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded layout model s3://layout/2025_02_18 on device mps with dtype torch.float16\n",
      "Loaded texify model s3://texify/2025_02_18 on device mps with dtype torch.float16\n",
      "Loaded recognition model s3://text_recognition/2025_02_18 on device mps with dtype torch.float16\n",
      "Loaded table recognition model s3://table_recognition/2025_02_18 on device mps with dtype torch.float16\n",
      "Loaded detection model s3://text_detection/2025_02_28 on device mps with dtype torch.float16\n",
      "Loaded detection model s3://inline_math_detection/2025_02_24 on device mps with dtype torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(83244) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(83245) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(83246) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(83247) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Recognizing layout: 100%|██████████| 128/128 [9:21:32<00:00, 263.23s/it]  \n",
      "Running OCR Error Detection: 100%|██████████| 191/191 [00:47<00:00,  3.99it/s]\n",
      "Detecting bboxes: 0it [00:00, ?it/s]\n",
      "Texify inference: 100%|██████████| 167/167 [2:08:55<00:00, 46.32s/it] \n",
      "Detecting bboxes: 0it [00:00, ?it/s]\n",
      "Recognizing tables: 100%|██████████| 11/11 [07:02<00:00, 38.39s/it]\n",
      "/Applications/anaconda3/lib/python3.10/site-packages/marker/renderers/html.py:47: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(document_output.html, 'html.parser')\n",
      "/Applications/anaconda3/lib/python3.10/site-packages/marker/renderers/html.py:79: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  ref.replace_with(BeautifulSoup(f\"{content}\", 'html.parser'))\n",
      "/Applications/anaconda3/lib/python3.10/site-packages/marker/renderers/html.py:47: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  soup = BeautifulSoup(document_output.html, 'html.parser')\n",
      "/Applications/anaconda3/lib/python3.10/site-packages/marker/renderers/html.py:79: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ref.replace_with(BeautifulSoup(f\"{content}\", 'html.parser'))\n"
     ]
    }
   ],
   "source": [
    "PATH = \"test_pdfs/BookAdvanced.pdf\"\n",
    "from marker.converters.pdf import PdfConverter\n",
    "from marker.models import create_model_dict\n",
    "from marker.output import text_from_rendered\n",
    "\n",
    "converter2 = PdfConverter(artifact_dict=create_model_dict(),)\n",
    "rendered2 = converter2(PATH)  # Fix: Use '=' instead of '-'\n",
    "text2, _, images2 = text_from_rendered(rendered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded layout model s3://layout/2025_02_18 on device mps with dtype torch.float16\n",
      "Loaded texify model s3://texify/2025_02_18 on device mps with dtype torch.float16\n",
      "Loaded recognition model s3://text_recognition/2025_02_18 on device mps with dtype torch.float16\n",
      "Loaded table recognition model s3://table_recognition/2025_02_18 on device mps with dtype torch.float16\n",
      "Loaded detection model s3://text_detection/2025_02_28 on device mps with dtype torch.float16\n",
      "Loaded detection model s3://inline_math_detection/2025_02_24 on device mps with dtype torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(24783) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(24784) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(24785) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(24786) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Recognizing layout: 100%|██████████| 74/74 [6:11:10<00:00, 300.95s/it]  \n",
      "Running OCR Error Detection: 100%|██████████| 111/111 [00:35<00:00,  3.10it/s]\n",
      "Detecting bboxes: 0it [00:00, ?it/s]\n",
      "Texify inference: 100%|██████████| 44/44 [08:04<00:00, 11.02s/it]\n",
      "Detecting bboxes: 0it [00:00, ?it/s]\n",
      "Recognizing tables: 100%|██████████| 8/8 [34:44<00:00, 260.51s/it]\n",
      "/Applications/anaconda3/lib/python3.10/site-packages/marker/renderers/html.py:47: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(document_output.html, 'html.parser')\n",
      "/Applications/anaconda3/lib/python3.10/site-packages/marker/renderers/html.py:79: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  ref.replace_with(BeautifulSoup(f\"{content}\", 'html.parser'))\n",
      "/Applications/anaconda3/lib/python3.10/site-packages/marker/renderers/html.py:47: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  soup = BeautifulSoup(document_output.html, 'html.parser')\n",
      "/Applications/anaconda3/lib/python3.10/site-packages/marker/renderers/html.py:79: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ref.replace_with(BeautifulSoup(f\"{content}\", 'html.parser'))\n"
     ]
    }
   ],
   "source": [
    "PATH = \"test_pdfs/ISLR_First_Printing.pdf\"\n",
    "from marker.converters.pdf import PdfConverter\n",
    "from marker.models import create_model_dict\n",
    "from marker.output import text_from_rendered\n",
    "\n",
    "converter3 = PdfConverter(artifact_dict=create_model_dict(),)\n",
    "rendered3 = converter2(PATH)  # Fix: Use '=' instead of '-'\n",
    "text3, _, images3 = text_from_rendered(rendered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = [text, text2, text3]\n",
    "image_list = [images, images2, images3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved file1.md\n",
      "Saved file2.md\n",
      "Saved file3.md\n"
     ]
    }
   ],
   "source": [
    "# Iterate through the text_list and save each text to a .md file\n",
    "for i, text in enumerate(text_list, start=1):\n",
    "    file_name = f\"file{i}.md\"  # Generate a unique file name\n",
    "    with open(file_name, \"w\") as file:\n",
    "        file.write(text)  # Write the content of the text to the file\n",
    "    print(f\"Saved {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "a bytes-like object is required, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_file\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Generate a unique file name\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m img_file:\n\u001b[0;32m----> 6\u001b[0m     \u001b[43mimg_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Write the content of the image to the file\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: a bytes-like object is required, not 'str'"
     ]
    }
   ],
   "source": [
    "# Iterate through the image_list and save each image to a .png file\n",
    "for i, images in enumerate(image_list, start=1):\n",
    "    for j, image in enumerate(images):\n",
    "        file_name = f\"image_file{i}_{j}.png\"  # Generate a unique file name\n",
    "        with open(file_name, \"wb\") as img_file:\n",
    "            img_file.write(image)  # Write the content of the image to the file\n",
    "        print(f\"Saved {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error decoding image 0 in file1: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 1 in file1: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 2 in file1: Incorrect padding\n",
      "Error decoding image 3 in file1: Incorrect padding\n",
      "Error decoding image 4 in file1: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 5 in file1: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 6 in file1: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 7 in file1: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 8 in file1: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 9 in file1: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 10 in file1: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 11 in file1: Incorrect padding\n",
      "Error decoding image 12 in file1: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 13 in file1: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 14 in file1: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 15 in file1: Incorrect padding\n",
      "Error decoding image 16 in file1: Incorrect padding\n",
      "Error decoding image 17 in file1: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 18 in file1: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 19 in file1: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 20 in file1: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 21 in file1: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 22 in file1: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 23 in file1: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 24 in file1: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 25 in file1: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 26 in file1: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 27 in file1: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 28 in file1: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 29 in file1: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 30 in file1: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 31 in file1: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 32 in file1: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 33 in file1: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 34 in file1: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 35 in file1: Incorrect padding\n",
      "Error decoding image 36 in file1: Incorrect padding\n",
      "Error decoding image 37 in file1: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 38 in file1: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 39 in file1: Incorrect padding\n",
      "Error decoding image 40 in file1: Incorrect padding\n",
      "Error decoding image 41 in file1: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 42 in file1: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 43 in file1: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 44 in file1: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 45 in file1: Incorrect padding\n",
      "Error decoding image 46 in file1: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 47 in file1: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 48 in file1: Incorrect padding\n",
      "Error decoding image 49 in file1: Incorrect padding\n",
      "Error decoding image 50 in file1: Incorrect padding\n",
      "Error decoding image 51 in file1: Incorrect padding\n",
      "Error decoding image 52 in file1: Incorrect padding\n",
      "Error decoding image 53 in file1: Incorrect padding\n",
      "Error decoding image 54 in file1: Incorrect padding\n",
      "Error decoding image 55 in file1: Incorrect padding\n",
      "Error decoding image 56 in file1: Incorrect padding\n",
      "Error decoding image 57 in file1: Incorrect padding\n",
      "Error decoding image 58 in file1: Incorrect padding\n",
      "Error decoding image 59 in file1: Incorrect padding\n",
      "Error decoding image 60 in file1: Incorrect padding\n",
      "Error decoding image 61 in file1: Incorrect padding\n",
      "Error decoding image 62 in file1: Incorrect padding\n",
      "Error decoding image 63 in file1: Incorrect padding\n",
      "Error decoding image 64 in file1: Incorrect padding\n",
      "Error decoding image 65 in file1: Incorrect padding\n",
      "Error decoding image 66 in file1: Incorrect padding\n",
      "Error decoding image 67 in file1: Incorrect padding\n",
      "Error decoding image 68 in file1: Incorrect padding\n",
      "Error decoding image 69 in file1: Incorrect padding\n",
      "Error decoding image 70 in file1: Incorrect padding\n",
      "Error decoding image 71 in file1: Incorrect padding\n",
      "Error decoding image 72 in file1: Incorrect padding\n",
      "Error decoding image 73 in file1: Incorrect padding\n",
      "Error decoding image 74 in file1: Incorrect padding\n",
      "Error decoding image 75 in file1: Incorrect padding\n",
      "Error decoding image 76 in file1: Incorrect padding\n",
      "Error decoding image 77 in file1: Incorrect padding\n",
      "Error decoding image 78 in file1: Incorrect padding\n",
      "Error decoding image 79 in file1: Incorrect padding\n",
      "Error decoding image 80 in file1: Incorrect padding\n",
      "Error decoding image 81 in file1: Incorrect padding\n",
      "Error decoding image 82 in file1: Incorrect padding\n",
      "Error decoding image 83 in file1: Incorrect padding\n",
      "Error decoding image 84 in file1: Incorrect padding\n",
      "Error decoding image 85 in file1: Incorrect padding\n",
      "Error decoding image 86 in file1: Incorrect padding\n",
      "Error decoding image 87 in file1: Incorrect padding\n",
      "Error decoding image 88 in file1: Incorrect padding\n",
      "Error decoding image 89 in file1: Incorrect padding\n",
      "Error decoding image 90 in file1: Incorrect padding\n",
      "Error decoding image 91 in file1: Incorrect padding\n",
      "Error decoding image 92 in file1: Incorrect padding\n",
      "Error decoding image 93 in file1: Incorrect padding\n",
      "Error decoding image 94 in file1: Incorrect padding\n",
      "Error decoding image 95 in file1: Incorrect padding\n",
      "Error decoding image 96 in file1: Incorrect padding\n",
      "Error decoding image 97 in file1: Incorrect padding\n",
      "Error decoding image 98 in file1: Incorrect padding\n",
      "Error decoding image 99 in file1: Incorrect padding\n",
      "Error decoding image 100 in file1: Incorrect padding\n",
      "Error decoding image 101 in file1: Incorrect padding\n",
      "Error decoding image 102 in file1: Incorrect padding\n",
      "Error decoding image 103 in file1: Incorrect padding\n",
      "Error decoding image 104 in file1: Incorrect padding\n",
      "Error decoding image 105 in file1: Incorrect padding\n",
      "Error decoding image 106 in file1: Incorrect padding\n",
      "Error decoding image 107 in file1: Incorrect padding\n",
      "Error decoding image 108 in file1: Incorrect padding\n",
      "Error decoding image 109 in file1: Incorrect padding\n",
      "Error decoding image 110 in file1: Incorrect padding\n",
      "Error decoding image 111 in file1: Incorrect padding\n",
      "Error decoding image 112 in file1: Incorrect padding\n",
      "Error decoding image 113 in file1: Incorrect padding\n",
      "Error decoding image 114 in file1: Incorrect padding\n",
      "Error decoding image 115 in file1: Incorrect padding\n",
      "Error decoding image 116 in file1: Incorrect padding\n",
      "Error decoding image 117 in file1: Incorrect padding\n",
      "Error decoding image 118 in file1: Incorrect padding\n",
      "Error decoding image 119 in file1: Incorrect padding\n",
      "Error decoding image 120 in file1: Incorrect padding\n",
      "Error decoding image 121 in file1: Incorrect padding\n",
      "Error decoding image 122 in file1: Incorrect padding\n",
      "Error decoding image 123 in file1: Incorrect padding\n",
      "Error decoding image 124 in file1: Incorrect padding\n",
      "Error decoding image 125 in file1: Incorrect padding\n",
      "Error decoding image 126 in file1: Incorrect padding\n",
      "Error decoding image 127 in file1: Incorrect padding\n",
      "Error decoding image 128 in file1: Incorrect padding\n",
      "Error decoding image 129 in file1: Incorrect padding\n",
      "Error decoding image 130 in file1: Incorrect padding\n",
      "Error decoding image 131 in file1: Incorrect padding\n",
      "Error decoding image 132 in file1: Incorrect padding\n",
      "Error decoding image 133 in file1: Incorrect padding\n",
      "Error decoding image 134 in file1: Incorrect padding\n",
      "Error decoding image 135 in file1: Incorrect padding\n",
      "Error decoding image 136 in file1: Incorrect padding\n",
      "Error decoding image 137 in file1: Incorrect padding\n",
      "Error decoding image 138 in file1: Incorrect padding\n",
      "Error decoding image 139 in file1: Incorrect padding\n",
      "Error decoding image 140 in file1: Incorrect padding\n",
      "Error decoding image 141 in file1: Incorrect padding\n",
      "Error decoding image 142 in file1: Incorrect padding\n",
      "Error decoding image 143 in file1: Incorrect padding\n",
      "Error decoding image 145 in file1: Incorrect padding\n",
      "Error decoding image 146 in file1: Incorrect padding\n",
      "Error decoding image 147 in file1: Incorrect padding\n",
      "Error decoding image 148 in file1: Incorrect padding\n",
      "Error decoding image 149 in file1: Incorrect padding\n",
      "Error decoding image 150 in file1: Incorrect padding\n",
      "Error decoding image 151 in file1: Incorrect padding\n",
      "Error decoding image 152 in file1: Incorrect padding\n",
      "Error decoding image 153 in file1: Incorrect padding\n",
      "Error decoding image 154 in file1: Incorrect padding\n",
      "Error decoding image 155 in file1: Incorrect padding\n",
      "Error decoding image 156 in file1: Incorrect padding\n",
      "Error decoding image 157 in file1: Incorrect padding\n",
      "Error decoding image 158 in file1: Incorrect padding\n",
      "Error decoding image 159 in file1: Incorrect padding\n",
      "Error decoding image 160 in file1: Incorrect padding\n",
      "Error decoding image 161 in file1: Incorrect padding\n",
      "Error decoding image 162 in file1: Incorrect padding\n",
      "Error decoding image 163 in file1: Incorrect padding\n",
      "Error decoding image 164 in file1: Incorrect padding\n",
      "Error decoding image 165 in file1: Incorrect padding\n",
      "Error decoding image 166 in file1: Incorrect padding\n",
      "Error decoding image 167 in file1: Incorrect padding\n",
      "Error decoding image 168 in file1: Incorrect padding\n",
      "Error decoding image 169 in file1: Incorrect padding\n",
      "Error decoding image 170 in file1: Incorrect padding\n",
      "Error decoding image 171 in file1: Incorrect padding\n",
      "Error decoding image 172 in file1: Incorrect padding\n",
      "Error decoding image 173 in file1: Incorrect padding\n",
      "Error decoding image 174 in file1: Incorrect padding\n",
      "Error decoding image 175 in file1: Incorrect padding\n",
      "Error decoding image 176 in file1: Incorrect padding\n",
      "Error decoding image 177 in file1: Incorrect padding\n",
      "Error decoding image 178 in file1: Incorrect padding\n",
      "Error decoding image 179 in file1: Incorrect padding\n",
      "Error decoding image 180 in file1: Incorrect padding\n",
      "Error decoding image 181 in file1: Incorrect padding\n",
      "Error decoding image 182 in file1: Incorrect padding\n",
      "Error decoding image 183 in file1: Incorrect padding\n",
      "Error decoding image 184 in file1: Incorrect padding\n",
      "Error decoding image 185 in file1: Incorrect padding\n",
      "Error decoding image 186 in file1: Incorrect padding\n",
      "Error decoding image 188 in file1: Incorrect padding\n",
      "Error decoding image 189 in file1: Incorrect padding\n",
      "Error decoding image 190 in file1: Incorrect padding\n",
      "Error decoding image 191 in file1: Incorrect padding\n",
      "Error decoding image 192 in file1: Incorrect padding\n",
      "Error decoding image 193 in file1: Incorrect padding\n",
      "Error decoding image 194 in file1: Incorrect padding\n",
      "Error decoding image 195 in file1: Incorrect padding\n",
      "Error decoding image 196 in file1: Incorrect padding\n",
      "Error decoding image 197 in file1: Incorrect padding\n",
      "Error decoding image 198 in file1: Incorrect padding\n",
      "Error decoding image 199 in file1: Incorrect padding\n",
      "Error decoding image 200 in file1: Incorrect padding\n",
      "Error decoding image 201 in file1: Incorrect padding\n",
      "Error decoding image 202 in file1: Incorrect padding\n",
      "Error decoding image 203 in file1: Incorrect padding\n",
      "Error decoding image 204 in file1: Incorrect padding\n",
      "Error decoding image 205 in file1: Incorrect padding\n",
      "Error decoding image 206 in file1: Incorrect padding\n",
      "Error decoding image 207 in file1: Incorrect padding\n",
      "Error decoding image 208 in file1: Incorrect padding\n",
      "Error decoding image 209 in file1: Incorrect padding\n",
      "Error decoding image 210 in file1: Incorrect padding\n",
      "Error decoding image 211 in file1: Incorrect padding\n",
      "Error decoding image 212 in file1: Incorrect padding\n",
      "Error decoding image 213 in file1: Incorrect padding\n",
      "Error decoding image 214 in file1: Incorrect padding\n",
      "Error decoding image 215 in file1: Incorrect padding\n",
      "Error decoding image 216 in file1: Incorrect padding\n",
      "Error decoding image 217 in file1: Incorrect padding\n",
      "Error decoding image 218 in file1: Incorrect padding\n",
      "Error decoding image 219 in file1: Incorrect padding\n",
      "Error decoding image 220 in file1: Incorrect padding\n",
      "Error decoding image 221 in file1: Incorrect padding\n",
      "Error decoding image 222 in file1: Incorrect padding\n",
      "Error decoding image 223 in file1: Incorrect padding\n",
      "Error decoding image 224 in file1: Incorrect padding\n",
      "Error decoding image 225 in file1: Incorrect padding\n",
      "Error decoding image 226 in file1: Incorrect padding\n",
      "Error decoding image 227 in file1: Incorrect padding\n",
      "Error decoding image 228 in file1: Incorrect padding\n",
      "Error decoding image 229 in file1: Incorrect padding\n",
      "Error decoding image 230 in file1: Incorrect padding\n",
      "Error decoding image 231 in file1: Incorrect padding\n",
      "Error decoding image 232 in file1: Incorrect padding\n",
      "Error decoding image 233 in file1: Incorrect padding\n",
      "Error decoding image 234 in file1: Incorrect padding\n",
      "Error decoding image 235 in file1: Incorrect padding\n",
      "Error decoding image 236 in file1: Incorrect padding\n",
      "Error decoding image 237 in file1: Incorrect padding\n",
      "Error decoding image 238 in file1: Incorrect padding\n",
      "Error decoding image 239 in file1: Incorrect padding\n",
      "Error decoding image 240 in file1: Incorrect padding\n",
      "Error decoding image 241 in file1: Incorrect padding\n",
      "Error decoding image 242 in file1: Incorrect padding\n",
      "Error decoding image 243 in file1: Incorrect padding\n",
      "Error decoding image 244 in file1: Incorrect padding\n",
      "Error decoding image 245 in file1: Incorrect padding\n",
      "Error decoding image 246 in file1: Incorrect padding\n",
      "Error decoding image 247 in file1: Incorrect padding\n",
      "Error decoding image 248 in file1: Incorrect padding\n",
      "Error decoding image 249 in file1: Incorrect padding\n",
      "Error decoding image 250 in file1: Incorrect padding\n",
      "Error decoding image 251 in file1: Incorrect padding\n",
      "Error decoding image 252 in file1: Incorrect padding\n",
      "Error decoding image 253 in file1: Incorrect padding\n",
      "Error decoding image 254 in file1: Incorrect padding\n",
      "Error decoding image 255 in file1: Incorrect padding\n",
      "Error decoding image 256 in file1: Incorrect padding\n",
      "Error decoding image 258 in file1: Incorrect padding\n",
      "Error decoding image 259 in file1: Incorrect padding\n",
      "Error decoding image 260 in file1: Incorrect padding\n",
      "Error decoding image 262 in file1: Incorrect padding\n",
      "Error decoding image 263 in file1: Incorrect padding\n",
      "Error decoding image 265 in file1: Incorrect padding\n",
      "Error decoding image 266 in file1: Incorrect padding\n",
      "Error decoding image 267 in file1: Incorrect padding\n",
      "Error decoding image 268 in file1: Incorrect padding\n",
      "Error decoding image 269 in file1: Incorrect padding\n",
      "Error decoding image 270 in file1: Incorrect padding\n",
      "Error decoding image 271 in file1: Incorrect padding\n",
      "Error decoding image 272 in file1: Incorrect padding\n",
      "Error decoding image 273 in file1: Incorrect padding\n",
      "Error decoding image 274 in file1: Incorrect padding\n",
      "Error decoding image 275 in file1: Incorrect padding\n",
      "Error decoding image 276 in file1: Incorrect padding\n",
      "Error decoding image 277 in file1: Incorrect padding\n",
      "Error decoding image 278 in file1: Incorrect padding\n",
      "Error decoding image 279 in file1: Incorrect padding\n",
      "Error decoding image 280 in file1: Incorrect padding\n",
      "Error decoding image 281 in file1: Incorrect padding\n",
      "Error decoding image 282 in file1: Incorrect padding\n",
      "Error decoding image 283 in file1: Incorrect padding\n",
      "Error decoding image 284 in file1: Incorrect padding\n",
      "Error decoding image 285 in file1: Incorrect padding\n",
      "Error decoding image 286 in file1: Incorrect padding\n",
      "Error decoding image 287 in file1: Incorrect padding\n",
      "Error decoding image 288 in file1: Incorrect padding\n",
      "Error decoding image 289 in file1: Incorrect padding\n",
      "Error decoding image 290 in file1: Incorrect padding\n",
      "Error decoding image 291 in file1: Incorrect padding\n",
      "Error decoding image 292 in file1: Incorrect padding\n",
      "Error decoding image 293 in file1: Incorrect padding\n",
      "Error decoding image 294 in file1: Incorrect padding\n",
      "Error decoding image 295 in file1: Incorrect padding\n",
      "Error decoding image 296 in file1: Incorrect padding\n",
      "Error decoding image 297 in file1: Incorrect padding\n",
      "Error decoding image 298 in file1: Incorrect padding\n",
      "Error decoding image 299 in file1: Incorrect padding\n",
      "Error decoding image 300 in file1: Incorrect padding\n",
      "Error decoding image 301 in file1: Incorrect padding\n",
      "Error decoding image 302 in file1: Incorrect padding\n",
      "Error decoding image 303 in file1: Incorrect padding\n",
      "Error decoding image 304 in file1: Incorrect padding\n",
      "Error decoding image 305 in file1: Incorrect padding\n",
      "Error decoding image 306 in file1: Incorrect padding\n",
      "Error decoding image 307 in file1: Incorrect padding\n",
      "Error decoding image 308 in file1: Incorrect padding\n",
      "Error decoding image 309 in file1: Incorrect padding\n",
      "Error decoding image 310 in file1: Incorrect padding\n",
      "Error decoding image 311 in file1: Incorrect padding\n",
      "Error decoding image 312 in file1: Incorrect padding\n",
      "Error decoding image 313 in file1: Incorrect padding\n",
      "Error decoding image 314 in file1: Incorrect padding\n",
      "Error decoding image 315 in file1: Incorrect padding\n",
      "Error decoding image 316 in file1: Incorrect padding\n",
      "Error decoding image 317 in file1: Incorrect padding\n",
      "Error decoding image 318 in file1: Incorrect padding\n",
      "Error decoding image 319 in file1: Incorrect padding\n",
      "Error decoding image 320 in file1: Incorrect padding\n",
      "Error decoding image 321 in file1: Incorrect padding\n",
      "Error decoding image 322 in file1: Incorrect padding\n",
      "Error decoding image 323 in file1: Incorrect padding\n",
      "Error decoding image 324 in file1: Incorrect padding\n",
      "Error decoding image 325 in file1: Incorrect padding\n",
      "Error decoding image 326 in file1: Incorrect padding\n",
      "Error decoding image 327 in file1: Incorrect padding\n",
      "Error decoding image 328 in file1: Incorrect padding\n",
      "Error decoding image 329 in file1: Incorrect padding\n",
      "Error decoding image 330 in file1: Incorrect padding\n",
      "Error decoding image 331 in file1: Incorrect padding\n",
      "Error decoding image 333 in file1: Incorrect padding\n",
      "Error decoding image 336 in file1: Incorrect padding\n",
      "Error decoding image 337 in file1: Incorrect padding\n",
      "Error decoding image 338 in file1: Incorrect padding\n",
      "Error decoding image 339 in file1: Incorrect padding\n",
      "Error decoding image 340 in file1: Incorrect padding\n",
      "Error decoding image 341 in file1: Incorrect padding\n",
      "Error decoding image 0 in file2: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 1 in file2: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 2 in file2: Incorrect padding\n",
      "Error decoding image 3 in file2: Incorrect padding\n",
      "Error decoding image 4 in file2: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 5 in file2: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 6 in file2: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 7 in file2: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 8 in file2: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 9 in file2: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 10 in file2: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 11 in file2: Incorrect padding\n",
      "Error decoding image 12 in file2: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 13 in file2: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 14 in file2: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 15 in file2: Incorrect padding\n",
      "Error decoding image 16 in file2: Incorrect padding\n",
      "Error decoding image 17 in file2: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 18 in file2: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 19 in file2: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 20 in file2: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 21 in file2: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 22 in file2: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 23 in file2: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 24 in file2: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 25 in file2: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 26 in file2: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 27 in file2: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 28 in file2: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 29 in file2: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 30 in file2: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 31 in file2: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 32 in file2: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 33 in file2: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 34 in file2: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 35 in file2: Incorrect padding\n",
      "Error decoding image 36 in file2: Incorrect padding\n",
      "Error decoding image 37 in file2: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 38 in file2: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 39 in file2: Incorrect padding\n",
      "Error decoding image 40 in file2: Incorrect padding\n",
      "Error decoding image 41 in file2: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 42 in file2: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 43 in file2: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 44 in file2: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 45 in file2: Incorrect padding\n",
      "Error decoding image 46 in file2: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 47 in file2: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 48 in file2: Incorrect padding\n",
      "Error decoding image 49 in file2: Incorrect padding\n",
      "Error decoding image 50 in file2: Incorrect padding\n",
      "Error decoding image 51 in file2: Incorrect padding\n",
      "Error decoding image 52 in file2: Incorrect padding\n",
      "Error decoding image 53 in file2: Incorrect padding\n",
      "Error decoding image 54 in file2: Incorrect padding\n",
      "Error decoding image 55 in file2: Incorrect padding\n",
      "Error decoding image 56 in file2: Incorrect padding\n",
      "Error decoding image 57 in file2: Incorrect padding\n",
      "Error decoding image 58 in file2: Incorrect padding\n",
      "Error decoding image 59 in file2: Incorrect padding\n",
      "Error decoding image 60 in file2: Incorrect padding\n",
      "Error decoding image 61 in file2: Incorrect padding\n",
      "Error decoding image 62 in file2: Incorrect padding\n",
      "Error decoding image 63 in file2: Incorrect padding\n",
      "Error decoding image 64 in file2: Incorrect padding\n",
      "Error decoding image 65 in file2: Incorrect padding\n",
      "Error decoding image 66 in file2: Incorrect padding\n",
      "Error decoding image 67 in file2: Incorrect padding\n",
      "Error decoding image 68 in file2: Incorrect padding\n",
      "Error decoding image 69 in file2: Incorrect padding\n",
      "Error decoding image 70 in file2: Incorrect padding\n",
      "Error decoding image 71 in file2: Incorrect padding\n",
      "Error decoding image 72 in file2: Incorrect padding\n",
      "Error decoding image 73 in file2: Incorrect padding\n",
      "Error decoding image 74 in file2: Incorrect padding\n",
      "Error decoding image 75 in file2: Incorrect padding\n",
      "Error decoding image 76 in file2: Incorrect padding\n",
      "Error decoding image 77 in file2: Incorrect padding\n",
      "Error decoding image 78 in file2: Incorrect padding\n",
      "Error decoding image 79 in file2: Incorrect padding\n",
      "Error decoding image 80 in file2: Incorrect padding\n",
      "Error decoding image 81 in file2: Incorrect padding\n",
      "Error decoding image 82 in file2: Incorrect padding\n",
      "Error decoding image 83 in file2: Incorrect padding\n",
      "Error decoding image 84 in file2: Incorrect padding\n",
      "Error decoding image 85 in file2: Incorrect padding\n",
      "Error decoding image 86 in file2: Incorrect padding\n",
      "Error decoding image 87 in file2: Incorrect padding\n",
      "Error decoding image 88 in file2: Incorrect padding\n",
      "Error decoding image 89 in file2: Incorrect padding\n",
      "Error decoding image 90 in file2: Incorrect padding\n",
      "Error decoding image 91 in file2: Incorrect padding\n",
      "Error decoding image 92 in file2: Incorrect padding\n",
      "Error decoding image 93 in file2: Incorrect padding\n",
      "Error decoding image 94 in file2: Incorrect padding\n",
      "Error decoding image 95 in file2: Incorrect padding\n",
      "Error decoding image 96 in file2: Incorrect padding\n",
      "Error decoding image 97 in file2: Incorrect padding\n",
      "Error decoding image 98 in file2: Incorrect padding\n",
      "Error decoding image 99 in file2: Incorrect padding\n",
      "Error decoding image 100 in file2: Incorrect padding\n",
      "Error decoding image 101 in file2: Incorrect padding\n",
      "Error decoding image 102 in file2: Incorrect padding\n",
      "Error decoding image 103 in file2: Incorrect padding\n",
      "Error decoding image 104 in file2: Incorrect padding\n",
      "Error decoding image 105 in file2: Incorrect padding\n",
      "Error decoding image 106 in file2: Incorrect padding\n",
      "Error decoding image 107 in file2: Incorrect padding\n",
      "Error decoding image 108 in file2: Incorrect padding\n",
      "Error decoding image 109 in file2: Incorrect padding\n",
      "Error decoding image 110 in file2: Incorrect padding\n",
      "Error decoding image 111 in file2: Incorrect padding\n",
      "Error decoding image 112 in file2: Incorrect padding\n",
      "Error decoding image 113 in file2: Incorrect padding\n",
      "Error decoding image 114 in file2: Incorrect padding\n",
      "Error decoding image 115 in file2: Incorrect padding\n",
      "Error decoding image 116 in file2: Incorrect padding\n",
      "Error decoding image 117 in file2: Incorrect padding\n",
      "Error decoding image 118 in file2: Incorrect padding\n",
      "Error decoding image 119 in file2: Incorrect padding\n",
      "Error decoding image 120 in file2: Incorrect padding\n",
      "Error decoding image 121 in file2: Incorrect padding\n",
      "Error decoding image 122 in file2: Incorrect padding\n",
      "Error decoding image 123 in file2: Incorrect padding\n",
      "Error decoding image 124 in file2: Incorrect padding\n",
      "Error decoding image 125 in file2: Incorrect padding\n",
      "Error decoding image 126 in file2: Incorrect padding\n",
      "Error decoding image 127 in file2: Incorrect padding\n",
      "Error decoding image 128 in file2: Incorrect padding\n",
      "Error decoding image 129 in file2: Incorrect padding\n",
      "Error decoding image 130 in file2: Incorrect padding\n",
      "Error decoding image 131 in file2: Incorrect padding\n",
      "Error decoding image 132 in file2: Incorrect padding\n",
      "Error decoding image 133 in file2: Incorrect padding\n",
      "Error decoding image 134 in file2: Incorrect padding\n",
      "Error decoding image 135 in file2: Incorrect padding\n",
      "Error decoding image 136 in file2: Incorrect padding\n",
      "Error decoding image 137 in file2: Incorrect padding\n",
      "Error decoding image 138 in file2: Incorrect padding\n",
      "Error decoding image 139 in file2: Incorrect padding\n",
      "Error decoding image 140 in file2: Incorrect padding\n",
      "Error decoding image 141 in file2: Incorrect padding\n",
      "Error decoding image 142 in file2: Incorrect padding\n",
      "Error decoding image 143 in file2: Incorrect padding\n",
      "Error decoding image 145 in file2: Incorrect padding\n",
      "Error decoding image 146 in file2: Incorrect padding\n",
      "Error decoding image 147 in file2: Incorrect padding\n",
      "Error decoding image 148 in file2: Incorrect padding\n",
      "Error decoding image 149 in file2: Incorrect padding\n",
      "Error decoding image 150 in file2: Incorrect padding\n",
      "Error decoding image 151 in file2: Incorrect padding\n",
      "Error decoding image 152 in file2: Incorrect padding\n",
      "Error decoding image 153 in file2: Incorrect padding\n",
      "Error decoding image 154 in file2: Incorrect padding\n",
      "Error decoding image 155 in file2: Incorrect padding\n",
      "Error decoding image 156 in file2: Incorrect padding\n",
      "Error decoding image 157 in file2: Incorrect padding\n",
      "Error decoding image 158 in file2: Incorrect padding\n",
      "Error decoding image 159 in file2: Incorrect padding\n",
      "Error decoding image 160 in file2: Incorrect padding\n",
      "Error decoding image 161 in file2: Incorrect padding\n",
      "Error decoding image 162 in file2: Incorrect padding\n",
      "Error decoding image 163 in file2: Incorrect padding\n",
      "Error decoding image 164 in file2: Incorrect padding\n",
      "Error decoding image 165 in file2: Incorrect padding\n",
      "Error decoding image 166 in file2: Incorrect padding\n",
      "Error decoding image 167 in file2: Incorrect padding\n",
      "Error decoding image 168 in file2: Incorrect padding\n",
      "Error decoding image 169 in file2: Incorrect padding\n",
      "Error decoding image 170 in file2: Incorrect padding\n",
      "Error decoding image 171 in file2: Incorrect padding\n",
      "Error decoding image 172 in file2: Incorrect padding\n",
      "Error decoding image 173 in file2: Incorrect padding\n",
      "Error decoding image 174 in file2: Incorrect padding\n",
      "Error decoding image 175 in file2: Incorrect padding\n",
      "Error decoding image 176 in file2: Incorrect padding\n",
      "Error decoding image 177 in file2: Incorrect padding\n",
      "Error decoding image 178 in file2: Incorrect padding\n",
      "Error decoding image 179 in file2: Incorrect padding\n",
      "Error decoding image 180 in file2: Incorrect padding\n",
      "Error decoding image 181 in file2: Incorrect padding\n",
      "Error decoding image 182 in file2: Incorrect padding\n",
      "Error decoding image 183 in file2: Incorrect padding\n",
      "Error decoding image 184 in file2: Incorrect padding\n",
      "Error decoding image 185 in file2: Incorrect padding\n",
      "Error decoding image 186 in file2: Incorrect padding\n",
      "Error decoding image 188 in file2: Incorrect padding\n",
      "Error decoding image 189 in file2: Incorrect padding\n",
      "Error decoding image 190 in file2: Incorrect padding\n",
      "Error decoding image 191 in file2: Incorrect padding\n",
      "Error decoding image 192 in file2: Incorrect padding\n",
      "Error decoding image 193 in file2: Incorrect padding\n",
      "Error decoding image 194 in file2: Incorrect padding\n",
      "Error decoding image 195 in file2: Incorrect padding\n",
      "Error decoding image 196 in file2: Incorrect padding\n",
      "Error decoding image 197 in file2: Incorrect padding\n",
      "Error decoding image 198 in file2: Incorrect padding\n",
      "Error decoding image 199 in file2: Incorrect padding\n",
      "Error decoding image 200 in file2: Incorrect padding\n",
      "Error decoding image 201 in file2: Incorrect padding\n",
      "Error decoding image 202 in file2: Incorrect padding\n",
      "Error decoding image 203 in file2: Incorrect padding\n",
      "Error decoding image 204 in file2: Incorrect padding\n",
      "Error decoding image 205 in file2: Incorrect padding\n",
      "Error decoding image 206 in file2: Incorrect padding\n",
      "Error decoding image 207 in file2: Incorrect padding\n",
      "Error decoding image 208 in file2: Incorrect padding\n",
      "Error decoding image 209 in file2: Incorrect padding\n",
      "Error decoding image 210 in file2: Incorrect padding\n",
      "Error decoding image 211 in file2: Incorrect padding\n",
      "Error decoding image 212 in file2: Incorrect padding\n",
      "Error decoding image 213 in file2: Incorrect padding\n",
      "Error decoding image 214 in file2: Incorrect padding\n",
      "Error decoding image 215 in file2: Incorrect padding\n",
      "Error decoding image 216 in file2: Incorrect padding\n",
      "Error decoding image 217 in file2: Incorrect padding\n",
      "Error decoding image 218 in file2: Incorrect padding\n",
      "Error decoding image 219 in file2: Incorrect padding\n",
      "Error decoding image 220 in file2: Incorrect padding\n",
      "Error decoding image 221 in file2: Incorrect padding\n",
      "Error decoding image 222 in file2: Incorrect padding\n",
      "Error decoding image 223 in file2: Incorrect padding\n",
      "Error decoding image 224 in file2: Incorrect padding\n",
      "Error decoding image 225 in file2: Incorrect padding\n",
      "Error decoding image 226 in file2: Incorrect padding\n",
      "Error decoding image 227 in file2: Incorrect padding\n",
      "Error decoding image 228 in file2: Incorrect padding\n",
      "Error decoding image 229 in file2: Incorrect padding\n",
      "Error decoding image 230 in file2: Incorrect padding\n",
      "Error decoding image 231 in file2: Incorrect padding\n",
      "Error decoding image 232 in file2: Incorrect padding\n",
      "Error decoding image 233 in file2: Incorrect padding\n",
      "Error decoding image 234 in file2: Incorrect padding\n",
      "Error decoding image 235 in file2: Incorrect padding\n",
      "Error decoding image 236 in file2: Incorrect padding\n",
      "Error decoding image 237 in file2: Incorrect padding\n",
      "Error decoding image 238 in file2: Incorrect padding\n",
      "Error decoding image 239 in file2: Incorrect padding\n",
      "Error decoding image 240 in file2: Incorrect padding\n",
      "Error decoding image 241 in file2: Incorrect padding\n",
      "Error decoding image 242 in file2: Incorrect padding\n",
      "Error decoding image 243 in file2: Incorrect padding\n",
      "Error decoding image 244 in file2: Incorrect padding\n",
      "Error decoding image 245 in file2: Incorrect padding\n",
      "Error decoding image 246 in file2: Incorrect padding\n",
      "Error decoding image 247 in file2: Incorrect padding\n",
      "Error decoding image 248 in file2: Incorrect padding\n",
      "Error decoding image 249 in file2: Incorrect padding\n",
      "Error decoding image 250 in file2: Incorrect padding\n",
      "Error decoding image 251 in file2: Incorrect padding\n",
      "Error decoding image 252 in file2: Incorrect padding\n",
      "Error decoding image 253 in file2: Incorrect padding\n",
      "Error decoding image 254 in file2: Incorrect padding\n",
      "Error decoding image 255 in file2: Incorrect padding\n",
      "Error decoding image 256 in file2: Incorrect padding\n",
      "Error decoding image 258 in file2: Incorrect padding\n",
      "Error decoding image 259 in file2: Incorrect padding\n",
      "Error decoding image 260 in file2: Incorrect padding\n",
      "Error decoding image 262 in file2: Incorrect padding\n",
      "Error decoding image 263 in file2: Incorrect padding\n",
      "Error decoding image 265 in file2: Incorrect padding\n",
      "Error decoding image 266 in file2: Incorrect padding\n",
      "Error decoding image 267 in file2: Incorrect padding\n",
      "Error decoding image 268 in file2: Incorrect padding\n",
      "Error decoding image 269 in file2: Incorrect padding\n",
      "Error decoding image 270 in file2: Incorrect padding\n",
      "Error decoding image 271 in file2: Incorrect padding\n",
      "Error decoding image 272 in file2: Incorrect padding\n",
      "Error decoding image 273 in file2: Incorrect padding\n",
      "Error decoding image 274 in file2: Incorrect padding\n",
      "Error decoding image 275 in file2: Incorrect padding\n",
      "Error decoding image 276 in file2: Incorrect padding\n",
      "Error decoding image 277 in file2: Incorrect padding\n",
      "Error decoding image 278 in file2: Incorrect padding\n",
      "Error decoding image 279 in file2: Incorrect padding\n",
      "Error decoding image 280 in file2: Incorrect padding\n",
      "Error decoding image 281 in file2: Incorrect padding\n",
      "Error decoding image 282 in file2: Incorrect padding\n",
      "Error decoding image 283 in file2: Incorrect padding\n",
      "Error decoding image 284 in file2: Incorrect padding\n",
      "Error decoding image 285 in file2: Incorrect padding\n",
      "Error decoding image 286 in file2: Incorrect padding\n",
      "Error decoding image 287 in file2: Incorrect padding\n",
      "Error decoding image 288 in file2: Incorrect padding\n",
      "Error decoding image 289 in file2: Incorrect padding\n",
      "Error decoding image 290 in file2: Incorrect padding\n",
      "Error decoding image 291 in file2: Incorrect padding\n",
      "Error decoding image 292 in file2: Incorrect padding\n",
      "Error decoding image 293 in file2: Incorrect padding\n",
      "Error decoding image 294 in file2: Incorrect padding\n",
      "Error decoding image 295 in file2: Incorrect padding\n",
      "Error decoding image 296 in file2: Incorrect padding\n",
      "Error decoding image 297 in file2: Incorrect padding\n",
      "Error decoding image 298 in file2: Incorrect padding\n",
      "Error decoding image 299 in file2: Incorrect padding\n",
      "Error decoding image 300 in file2: Incorrect padding\n",
      "Error decoding image 301 in file2: Incorrect padding\n",
      "Error decoding image 302 in file2: Incorrect padding\n",
      "Error decoding image 303 in file2: Incorrect padding\n",
      "Error decoding image 304 in file2: Incorrect padding\n",
      "Error decoding image 305 in file2: Incorrect padding\n",
      "Error decoding image 306 in file2: Incorrect padding\n",
      "Error decoding image 307 in file2: Incorrect padding\n",
      "Error decoding image 308 in file2: Incorrect padding\n",
      "Error decoding image 309 in file2: Incorrect padding\n",
      "Error decoding image 310 in file2: Incorrect padding\n",
      "Error decoding image 311 in file2: Incorrect padding\n",
      "Error decoding image 312 in file2: Incorrect padding\n",
      "Error decoding image 313 in file2: Incorrect padding\n",
      "Error decoding image 314 in file2: Incorrect padding\n",
      "Error decoding image 315 in file2: Incorrect padding\n",
      "Error decoding image 316 in file2: Incorrect padding\n",
      "Error decoding image 317 in file2: Incorrect padding\n",
      "Error decoding image 318 in file2: Incorrect padding\n",
      "Error decoding image 319 in file2: Incorrect padding\n",
      "Error decoding image 320 in file2: Incorrect padding\n",
      "Error decoding image 321 in file2: Incorrect padding\n",
      "Error decoding image 322 in file2: Incorrect padding\n",
      "Error decoding image 323 in file2: Incorrect padding\n",
      "Error decoding image 324 in file2: Incorrect padding\n",
      "Error decoding image 325 in file2: Incorrect padding\n",
      "Error decoding image 326 in file2: Incorrect padding\n",
      "Error decoding image 327 in file2: Incorrect padding\n",
      "Error decoding image 328 in file2: Incorrect padding\n",
      "Error decoding image 329 in file2: Incorrect padding\n",
      "Error decoding image 330 in file2: Incorrect padding\n",
      "Error decoding image 331 in file2: Incorrect padding\n",
      "Error decoding image 333 in file2: Incorrect padding\n",
      "Error decoding image 336 in file2: Incorrect padding\n",
      "Error decoding image 337 in file2: Incorrect padding\n",
      "Error decoding image 338 in file2: Incorrect padding\n",
      "Error decoding image 339 in file2: Incorrect padding\n",
      "Error decoding image 340 in file2: Incorrect padding\n",
      "Error decoding image 341 in file2: Incorrect padding\n",
      "Error decoding image 0 in file3: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 1 in file3: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 2 in file3: Incorrect padding\n",
      "Error decoding image 3 in file3: Incorrect padding\n",
      "Error decoding image 4 in file3: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 5 in file3: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 6 in file3: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 7 in file3: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 8 in file3: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 9 in file3: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 10 in file3: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 11 in file3: Incorrect padding\n",
      "Error decoding image 12 in file3: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 13 in file3: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 14 in file3: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 15 in file3: Incorrect padding\n",
      "Error decoding image 16 in file3: Incorrect padding\n",
      "Error decoding image 17 in file3: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 18 in file3: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 19 in file3: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 20 in file3: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 21 in file3: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 22 in file3: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 23 in file3: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 24 in file3: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 25 in file3: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 26 in file3: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 27 in file3: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 28 in file3: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 29 in file3: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 30 in file3: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 31 in file3: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 32 in file3: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 33 in file3: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 34 in file3: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 35 in file3: Incorrect padding\n",
      "Error decoding image 36 in file3: Incorrect padding\n",
      "Error decoding image 37 in file3: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 38 in file3: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 39 in file3: Incorrect padding\n",
      "Error decoding image 40 in file3: Incorrect padding\n",
      "Error decoding image 41 in file3: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 42 in file3: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 43 in file3: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 44 in file3: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 45 in file3: Incorrect padding\n",
      "Error decoding image 46 in file3: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 47 in file3: Invalid base64-encoded string: number of data characters (17) cannot be 1 more than a multiple of 4\n",
      "Error decoding image 48 in file3: Incorrect padding\n",
      "Error decoding image 49 in file3: Incorrect padding\n",
      "Error decoding image 50 in file3: Incorrect padding\n",
      "Error decoding image 51 in file3: Incorrect padding\n",
      "Error decoding image 52 in file3: Incorrect padding\n",
      "Error decoding image 53 in file3: Incorrect padding\n",
      "Error decoding image 54 in file3: Incorrect padding\n",
      "Error decoding image 55 in file3: Incorrect padding\n",
      "Error decoding image 56 in file3: Incorrect padding\n",
      "Error decoding image 57 in file3: Incorrect padding\n",
      "Error decoding image 58 in file3: Incorrect padding\n",
      "Error decoding image 59 in file3: Incorrect padding\n",
      "Error decoding image 60 in file3: Incorrect padding\n",
      "Error decoding image 61 in file3: Incorrect padding\n",
      "Error decoding image 62 in file3: Incorrect padding\n",
      "Error decoding image 63 in file3: Incorrect padding\n",
      "Error decoding image 64 in file3: Incorrect padding\n",
      "Error decoding image 65 in file3: Incorrect padding\n",
      "Error decoding image 66 in file3: Incorrect padding\n",
      "Error decoding image 67 in file3: Incorrect padding\n",
      "Error decoding image 68 in file3: Incorrect padding\n",
      "Error decoding image 69 in file3: Incorrect padding\n",
      "Error decoding image 70 in file3: Incorrect padding\n",
      "Error decoding image 71 in file3: Incorrect padding\n",
      "Error decoding image 72 in file3: Incorrect padding\n",
      "Error decoding image 73 in file3: Incorrect padding\n",
      "Error decoding image 74 in file3: Incorrect padding\n",
      "Error decoding image 75 in file3: Incorrect padding\n",
      "Error decoding image 76 in file3: Incorrect padding\n",
      "Error decoding image 77 in file3: Incorrect padding\n",
      "Error decoding image 78 in file3: Incorrect padding\n",
      "Error decoding image 79 in file3: Incorrect padding\n",
      "Error decoding image 80 in file3: Incorrect padding\n",
      "Error decoding image 81 in file3: Incorrect padding\n",
      "Error decoding image 82 in file3: Incorrect padding\n",
      "Error decoding image 83 in file3: Incorrect padding\n",
      "Error decoding image 84 in file3: Incorrect padding\n",
      "Error decoding image 85 in file3: Incorrect padding\n",
      "Error decoding image 86 in file3: Incorrect padding\n",
      "Error decoding image 87 in file3: Incorrect padding\n",
      "Error decoding image 88 in file3: Incorrect padding\n",
      "Error decoding image 89 in file3: Incorrect padding\n",
      "Error decoding image 90 in file3: Incorrect padding\n",
      "Error decoding image 91 in file3: Incorrect padding\n",
      "Error decoding image 92 in file3: Incorrect padding\n",
      "Error decoding image 93 in file3: Incorrect padding\n",
      "Error decoding image 94 in file3: Incorrect padding\n",
      "Error decoding image 95 in file3: Incorrect padding\n",
      "Error decoding image 96 in file3: Incorrect padding\n",
      "Error decoding image 97 in file3: Incorrect padding\n",
      "Error decoding image 98 in file3: Incorrect padding\n",
      "Error decoding image 99 in file3: Incorrect padding\n",
      "Error decoding image 100 in file3: Incorrect padding\n",
      "Error decoding image 101 in file3: Incorrect padding\n",
      "Error decoding image 102 in file3: Incorrect padding\n",
      "Error decoding image 103 in file3: Incorrect padding\n",
      "Error decoding image 104 in file3: Incorrect padding\n",
      "Error decoding image 105 in file3: Incorrect padding\n",
      "Error decoding image 106 in file3: Incorrect padding\n",
      "Error decoding image 107 in file3: Incorrect padding\n",
      "Error decoding image 108 in file3: Incorrect padding\n",
      "Error decoding image 109 in file3: Incorrect padding\n",
      "Error decoding image 110 in file3: Incorrect padding\n",
      "Error decoding image 111 in file3: Incorrect padding\n",
      "Error decoding image 112 in file3: Incorrect padding\n",
      "Error decoding image 113 in file3: Incorrect padding\n",
      "Error decoding image 114 in file3: Incorrect padding\n",
      "Error decoding image 115 in file3: Incorrect padding\n",
      "Error decoding image 116 in file3: Incorrect padding\n",
      "Error decoding image 117 in file3: Incorrect padding\n",
      "Error decoding image 118 in file3: Incorrect padding\n",
      "Error decoding image 119 in file3: Incorrect padding\n",
      "Error decoding image 120 in file3: Incorrect padding\n",
      "Error decoding image 121 in file3: Incorrect padding\n",
      "Error decoding image 122 in file3: Incorrect padding\n",
      "Error decoding image 123 in file3: Incorrect padding\n",
      "Error decoding image 124 in file3: Incorrect padding\n",
      "Error decoding image 125 in file3: Incorrect padding\n",
      "Error decoding image 126 in file3: Incorrect padding\n",
      "Error decoding image 127 in file3: Incorrect padding\n",
      "Error decoding image 128 in file3: Incorrect padding\n",
      "Error decoding image 129 in file3: Incorrect padding\n",
      "Error decoding image 130 in file3: Incorrect padding\n",
      "Error decoding image 131 in file3: Incorrect padding\n",
      "Error decoding image 132 in file3: Incorrect padding\n",
      "Error decoding image 133 in file3: Incorrect padding\n",
      "Error decoding image 134 in file3: Incorrect padding\n",
      "Error decoding image 135 in file3: Incorrect padding\n",
      "Error decoding image 136 in file3: Incorrect padding\n",
      "Error decoding image 137 in file3: Incorrect padding\n",
      "Error decoding image 138 in file3: Incorrect padding\n",
      "Error decoding image 139 in file3: Incorrect padding\n",
      "Error decoding image 140 in file3: Incorrect padding\n",
      "Error decoding image 141 in file3: Incorrect padding\n",
      "Error decoding image 142 in file3: Incorrect padding\n",
      "Error decoding image 143 in file3: Incorrect padding\n",
      "Error decoding image 145 in file3: Incorrect padding\n",
      "Error decoding image 146 in file3: Incorrect padding\n",
      "Error decoding image 147 in file3: Incorrect padding\n",
      "Error decoding image 148 in file3: Incorrect padding\n",
      "Error decoding image 149 in file3: Incorrect padding\n",
      "Error decoding image 150 in file3: Incorrect padding\n",
      "Error decoding image 151 in file3: Incorrect padding\n",
      "Error decoding image 152 in file3: Incorrect padding\n",
      "Error decoding image 153 in file3: Incorrect padding\n",
      "Error decoding image 154 in file3: Incorrect padding\n",
      "Error decoding image 155 in file3: Incorrect padding\n",
      "Error decoding image 156 in file3: Incorrect padding\n",
      "Error decoding image 157 in file3: Incorrect padding\n",
      "Error decoding image 158 in file3: Incorrect padding\n",
      "Error decoding image 159 in file3: Incorrect padding\n",
      "Error decoding image 160 in file3: Incorrect padding\n",
      "Error decoding image 161 in file3: Incorrect padding\n",
      "Error decoding image 162 in file3: Incorrect padding\n",
      "Error decoding image 163 in file3: Incorrect padding\n",
      "Error decoding image 164 in file3: Incorrect padding\n",
      "Error decoding image 165 in file3: Incorrect padding\n",
      "Error decoding image 166 in file3: Incorrect padding\n",
      "Error decoding image 167 in file3: Incorrect padding\n",
      "Error decoding image 168 in file3: Incorrect padding\n",
      "Error decoding image 169 in file3: Incorrect padding\n",
      "Error decoding image 170 in file3: Incorrect padding\n",
      "Error decoding image 171 in file3: Incorrect padding\n",
      "Error decoding image 172 in file3: Incorrect padding\n",
      "Error decoding image 173 in file3: Incorrect padding\n",
      "Error decoding image 174 in file3: Incorrect padding\n",
      "Error decoding image 175 in file3: Incorrect padding\n",
      "Error decoding image 176 in file3: Incorrect padding\n",
      "Error decoding image 177 in file3: Incorrect padding\n",
      "Error decoding image 178 in file3: Incorrect padding\n",
      "Error decoding image 179 in file3: Incorrect padding\n",
      "Error decoding image 180 in file3: Incorrect padding\n",
      "Error decoding image 181 in file3: Incorrect padding\n",
      "Error decoding image 182 in file3: Incorrect padding\n",
      "Error decoding image 183 in file3: Incorrect padding\n",
      "Error decoding image 184 in file3: Incorrect padding\n",
      "Error decoding image 185 in file3: Incorrect padding\n",
      "Error decoding image 186 in file3: Incorrect padding\n",
      "Error decoding image 188 in file3: Incorrect padding\n",
      "Error decoding image 189 in file3: Incorrect padding\n",
      "Error decoding image 190 in file3: Incorrect padding\n",
      "Error decoding image 191 in file3: Incorrect padding\n",
      "Error decoding image 192 in file3: Incorrect padding\n",
      "Error decoding image 193 in file3: Incorrect padding\n",
      "Error decoding image 194 in file3: Incorrect padding\n",
      "Error decoding image 195 in file3: Incorrect padding\n",
      "Error decoding image 196 in file3: Incorrect padding\n",
      "Error decoding image 197 in file3: Incorrect padding\n",
      "Error decoding image 198 in file3: Incorrect padding\n",
      "Error decoding image 199 in file3: Incorrect padding\n",
      "Error decoding image 200 in file3: Incorrect padding\n",
      "Error decoding image 201 in file3: Incorrect padding\n",
      "Error decoding image 202 in file3: Incorrect padding\n",
      "Error decoding image 203 in file3: Incorrect padding\n",
      "Error decoding image 204 in file3: Incorrect padding\n",
      "Error decoding image 205 in file3: Incorrect padding\n",
      "Error decoding image 206 in file3: Incorrect padding\n",
      "Error decoding image 207 in file3: Incorrect padding\n",
      "Error decoding image 208 in file3: Incorrect padding\n",
      "Error decoding image 209 in file3: Incorrect padding\n",
      "Error decoding image 210 in file3: Incorrect padding\n",
      "Error decoding image 211 in file3: Incorrect padding\n",
      "Error decoding image 212 in file3: Incorrect padding\n",
      "Error decoding image 213 in file3: Incorrect padding\n",
      "Error decoding image 214 in file3: Incorrect padding\n",
      "Error decoding image 215 in file3: Incorrect padding\n",
      "Error decoding image 216 in file3: Incorrect padding\n",
      "Error decoding image 217 in file3: Incorrect padding\n",
      "Error decoding image 218 in file3: Incorrect padding\n",
      "Error decoding image 219 in file3: Incorrect padding\n",
      "Error decoding image 220 in file3: Incorrect padding\n",
      "Error decoding image 221 in file3: Incorrect padding\n",
      "Error decoding image 222 in file3: Incorrect padding\n",
      "Error decoding image 223 in file3: Incorrect padding\n",
      "Error decoding image 224 in file3: Incorrect padding\n",
      "Error decoding image 225 in file3: Incorrect padding\n",
      "Error decoding image 226 in file3: Incorrect padding\n",
      "Error decoding image 227 in file3: Incorrect padding\n",
      "Error decoding image 228 in file3: Incorrect padding\n",
      "Error decoding image 229 in file3: Incorrect padding\n",
      "Error decoding image 230 in file3: Incorrect padding\n",
      "Error decoding image 231 in file3: Incorrect padding\n",
      "Error decoding image 232 in file3: Incorrect padding\n",
      "Error decoding image 233 in file3: Incorrect padding\n",
      "Error decoding image 234 in file3: Incorrect padding\n",
      "Error decoding image 235 in file3: Incorrect padding\n",
      "Error decoding image 236 in file3: Incorrect padding\n",
      "Error decoding image 237 in file3: Incorrect padding\n",
      "Error decoding image 238 in file3: Incorrect padding\n",
      "Error decoding image 239 in file3: Incorrect padding\n",
      "Error decoding image 240 in file3: Incorrect padding\n",
      "Error decoding image 241 in file3: Incorrect padding\n",
      "Error decoding image 242 in file3: Incorrect padding\n",
      "Error decoding image 243 in file3: Incorrect padding\n",
      "Error decoding image 244 in file3: Incorrect padding\n",
      "Error decoding image 245 in file3: Incorrect padding\n",
      "Error decoding image 246 in file3: Incorrect padding\n",
      "Error decoding image 247 in file3: Incorrect padding\n",
      "Error decoding image 248 in file3: Incorrect padding\n",
      "Error decoding image 249 in file3: Incorrect padding\n",
      "Error decoding image 250 in file3: Incorrect padding\n",
      "Error decoding image 251 in file3: Incorrect padding\n",
      "Error decoding image 252 in file3: Incorrect padding\n",
      "Error decoding image 253 in file3: Incorrect padding\n",
      "Error decoding image 254 in file3: Incorrect padding\n",
      "Error decoding image 255 in file3: Incorrect padding\n",
      "Error decoding image 256 in file3: Incorrect padding\n",
      "Error decoding image 258 in file3: Incorrect padding\n",
      "Error decoding image 259 in file3: Incorrect padding\n",
      "Error decoding image 260 in file3: Incorrect padding\n",
      "Error decoding image 262 in file3: Incorrect padding\n",
      "Error decoding image 263 in file3: Incorrect padding\n",
      "Error decoding image 265 in file3: Incorrect padding\n",
      "Error decoding image 266 in file3: Incorrect padding\n",
      "Error decoding image 267 in file3: Incorrect padding\n",
      "Error decoding image 268 in file3: Incorrect padding\n",
      "Error decoding image 269 in file3: Incorrect padding\n",
      "Error decoding image 270 in file3: Incorrect padding\n",
      "Error decoding image 271 in file3: Incorrect padding\n",
      "Error decoding image 272 in file3: Incorrect padding\n",
      "Error decoding image 273 in file3: Incorrect padding\n",
      "Error decoding image 274 in file3: Incorrect padding\n",
      "Error decoding image 275 in file3: Incorrect padding\n",
      "Error decoding image 276 in file3: Incorrect padding\n",
      "Error decoding image 277 in file3: Incorrect padding\n",
      "Error decoding image 278 in file3: Incorrect padding\n",
      "Error decoding image 279 in file3: Incorrect padding\n",
      "Error decoding image 280 in file3: Incorrect padding\n",
      "Error decoding image 281 in file3: Incorrect padding\n",
      "Error decoding image 282 in file3: Incorrect padding\n",
      "Error decoding image 283 in file3: Incorrect padding\n",
      "Error decoding image 284 in file3: Incorrect padding\n",
      "Error decoding image 285 in file3: Incorrect padding\n",
      "Error decoding image 286 in file3: Incorrect padding\n",
      "Error decoding image 287 in file3: Incorrect padding\n",
      "Error decoding image 288 in file3: Incorrect padding\n",
      "Error decoding image 289 in file3: Incorrect padding\n",
      "Error decoding image 290 in file3: Incorrect padding\n",
      "Error decoding image 291 in file3: Incorrect padding\n",
      "Error decoding image 292 in file3: Incorrect padding\n",
      "Error decoding image 293 in file3: Incorrect padding\n",
      "Error decoding image 294 in file3: Incorrect padding\n",
      "Error decoding image 295 in file3: Incorrect padding\n",
      "Error decoding image 296 in file3: Incorrect padding\n",
      "Error decoding image 297 in file3: Incorrect padding\n",
      "Error decoding image 298 in file3: Incorrect padding\n",
      "Error decoding image 299 in file3: Incorrect padding\n",
      "Error decoding image 300 in file3: Incorrect padding\n",
      "Error decoding image 301 in file3: Incorrect padding\n",
      "Error decoding image 302 in file3: Incorrect padding\n",
      "Error decoding image 303 in file3: Incorrect padding\n",
      "Error decoding image 304 in file3: Incorrect padding\n",
      "Error decoding image 305 in file3: Incorrect padding\n",
      "Error decoding image 306 in file3: Incorrect padding\n",
      "Error decoding image 307 in file3: Incorrect padding\n",
      "Error decoding image 308 in file3: Incorrect padding\n",
      "Error decoding image 309 in file3: Incorrect padding\n",
      "Error decoding image 310 in file3: Incorrect padding\n",
      "Error decoding image 311 in file3: Incorrect padding\n",
      "Error decoding image 312 in file3: Incorrect padding\n",
      "Error decoding image 313 in file3: Incorrect padding\n",
      "Error decoding image 314 in file3: Incorrect padding\n",
      "Error decoding image 315 in file3: Incorrect padding\n",
      "Error decoding image 316 in file3: Incorrect padding\n",
      "Error decoding image 317 in file3: Incorrect padding\n",
      "Error decoding image 318 in file3: Incorrect padding\n",
      "Error decoding image 319 in file3: Incorrect padding\n",
      "Error decoding image 320 in file3: Incorrect padding\n",
      "Error decoding image 321 in file3: Incorrect padding\n",
      "Error decoding image 322 in file3: Incorrect padding\n",
      "Error decoding image 323 in file3: Incorrect padding\n",
      "Error decoding image 324 in file3: Incorrect padding\n",
      "Error decoding image 325 in file3: Incorrect padding\n",
      "Error decoding image 326 in file3: Incorrect padding\n",
      "Error decoding image 327 in file3: Incorrect padding\n",
      "Error decoding image 328 in file3: Incorrect padding\n",
      "Error decoding image 329 in file3: Incorrect padding\n",
      "Error decoding image 330 in file3: Incorrect padding\n",
      "Error decoding image 331 in file3: Incorrect padding\n",
      "Error decoding image 333 in file3: Incorrect padding\n",
      "Error decoding image 336 in file3: Incorrect padding\n",
      "Error decoding image 337 in file3: Incorrect padding\n",
      "Error decoding image 338 in file3: Incorrect padding\n",
      "Error decoding image 339 in file3: Incorrect padding\n",
      "Error decoding image 340 in file3: Incorrect padding\n",
      "Error decoding image 341 in file3: Incorrect padding\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import base64\n",
    "\n",
    "# Save the text and images to a structured folder\n",
    "for i, (text, images) in enumerate(zip(text_list, image_list), start=1):\n",
    "    folder_name = f\"scrapped_url_md/file{i}\"\n",
    "    os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "    # Save text to a .md file\n",
    "    with open(os.path.join(folder_name, \"text.md\"), \"w\") as file:\n",
    "        file.write(text)\n",
    "\n",
    "    # Save each image to the folder\n",
    "    for j, image in enumerate(images):\n",
    "        image_file_name = os.path.join(folder_name, f\"image_{j}.png\")\n",
    "        with open(image_file_name, \"wb\") as img_file:\n",
    "            # Decode the image string to bytes if necessary\n",
    "            if isinstance(image, str):\n",
    "                try:\n",
    "                    # Ensure the string length is a multiple of 4 by padding with '='\n",
    "                    missing_padding = len(image) % 4\n",
    "                    if missing_padding:\n",
    "                        image += \"=\" * (4 - missing_padding)\n",
    "                    image = base64.b64decode(image)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error decoding image {j} in file{i}: {e}\")\n",
    "                    continue\n",
    "            img_file.write(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved image_text_files/image1.txt\n",
      "Saved image_text_files/image2.txt\n",
      "Saved image_text_files/image3.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Example dictionary containing image content\n",
    "image_dict = {\n",
    "    \"image1\": b\"binary_image_data_1\",\n",
    "    \"image2\": b\"binary_image_data_2\",\n",
    "    \"image3\": b\"binary_image_data_3\",\n",
    "}\n",
    "\n",
    "# Directory to store the .txt files\n",
    "output_dir = \"image_text_files\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Iterate through the dictionary and save each image content as a string in a .txt file\n",
    "for image_name, image_content in image_dict.items():\n",
    "    file_name = os.path.join(output_dir, f\"{image_name}.txt\")\n",
    "    with open(file_name, \"w\") as file:\n",
    "        # Convert image content (bytes) to a string and write to the file\n",
    "        file.write(image_content.decode(\"latin1\"))  # Use \"latin1\" to preserve binary data\n",
    "    print(f\"Saved {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved image_json_files/image_1.json\n",
      "Saved image_json_files/image_2.json\n",
      "Saved image_json_files/image_3.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Directory to store the .json files\n",
    "output_dir = \"image_json_files\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Iterate through the image list and save each dictionary as a JSON file\n",
    "for i, image_dict in enumerate(image_list, start=1):\n",
    "    file_name = os.path.join(output_dir, f\"image_{i}.json\")\n",
    "    with open(file_name, \"w\") as json_file:\n",
    "        json.dump(image_dict, json_file, indent=4)  # Convert dict to JSON and save\n",
    "    print(f\"Saved {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
