- [Deep Learning Tutorial](https://www.geeksforgeeks.org/deep-learning-tutorial/)
- [Data Analysis Tutorial](https://www.geeksforgeeks.org/data-analysis-tutorial/)
- [Python â€“ Data visualization tutorial](https://www.geeksforgeeks.org/python-data-visualization-tutorial/)
- [NumPy](https://www.geeksforgeeks.org/numpy-tutorial/)
- [Pandas](https://www.geeksforgeeks.org/pandas-tutorial/)
- [OpenCV](https://www.geeksforgeeks.org/opencv-python-tutorial/)
- [R](https://www.geeksforgeeks.org/r-tutorial/)
- [Machine Learning Tutorial](https://www.geeksforgeeks.org/machine-learning/)
- [Machine Learning Projects](https://www.geeksforgeeks.org/machine-learning-projects/)_)
- [Machine Learning Interview Questions](https://www.geeksforgeeks.org/machine-learning-interview-questions/)_)
- [Machine Learning Mathematics](https://www.geeksforgeeks.org/machine-learning-mathematics/)
- [Deep Learning Project](https://www.geeksforgeeks.org/5-deep-learning-project-ideas-for-beginners/)_)
- [Deep Learning Interview Questions](https://www.geeksforgeeks.org/deep-learning-interview-questions/)_)
- [Computer Vision Tutorial](https://www.geeksforgeeks.org/computer-vision/)
- [Computer Vision Projects](https://www.geeksforgeeks.org/computer-vision-projects/)
- [NLP](https://www.geeksforgeeks.org/natural-language-processing-nlp-tutorial/)
- [NLP Project](https://www.geeksforgeeks.org/nlp-project-ideas-for-beginners/))
- [NLP Interview Questions](https://www.geeksforgeeks.org/nlp-interview-questions/))
- [Statistics with Python](https://www.geeksforgeeks.org/statistics-with-python/)
- [100 Days of Machine Learning](https://www.geeksforgeeks.org/100-days-of-machine-learning/)

Sign In

▲

[Open In App](https://geeksforgeeksapp.page.link/?link=https://www.geeksforgeeks.org/denoising-autoencoders-in-machine-learning/?type%3Darticle%26id%3D1127348&apn=free.programming.programming&isi=1641848816&ibi=org.geeksforgeeks.GeeksforGeeksDev&efr=1)

[Next Article:\\
\\
Monte Carlo Policy Evaluation\\
\\
![Next article icon](https://media.geeksforgeeks.org/auth-dashboard-uploads/ep_right.svg)](https://www.geeksforgeeks.org/monte-carlo-policy-evaluation/)

# Denoising AutoEncoders In Machine Learning

Last Updated : 30 Dec, 2024

Comments

Improve

Suggest changes

2 Likes

Like

Report

**Autoencoders** are types of **neural network architecture** used for **unsupervised learning**. The architecture consists of an encoder and a decoder. The encoder encodes the input data into a lower dimensional space while the decoder decodes the encoded data back to the original input. The network is trained to minimize the difference between decoded data and input. Autoencoders have the risk of becoming an Identify function meaning the output equals the input which makes the whole neural network of autoencoders useless. This generally happens when there are more nodes in the hidden layer than there are inputs.

## Denoising Autoencoder (DAE)

Now, a **denoising autoencoder** is a modification of the original autoencoder in which instead of giving the original input we give a corrupted or noisy version of input to the encoder while decoder loss is calculated concerning original input only. This results in efficient learning of autoencoders and the risk of autoencoder becoming an identity function is significantly reduced.

### Architecture of DAE

The denoising autoencoder (DAE) architecture resembles a standard [autoencoder](https://www.geeksforgeeks.org/auto-encoders/) and consists of two main components:

#### Encoder:

- The encoder is a neural network with one or more hidden layers.
- It receives noisy input data instead of the original input and generates an encoding in a low-dimensional space.
- There are several ways to generate a corrupted input. The most common being adding a Gaussian noise or randomly masking some of the inputs.

#### Decoder:

- Similar to encoders, decoders are implemented as neural networks with one or more hidden layers.
- It takes the encoding generated by the encoder as input and reconstructs the original data.
- When calculating the Loss function it compares the output values with the original input, not with the corrupted input.

### What DAE Learns?

The above architecture of using a corrupted input helps decrease the risk of overfitting and prevents the DAE from becoming an identity function.

- If DAEs are trained with partially corrupted inputs (e.g., with masking values), they learn to impute or fill in missing information during the reconstruction process. This makes them useful for tasks involving incomplete datasets.
- If DAEs are trained with partially noisy inputs (gaussian noise) DAEs tend to generalize well to unseen, real-world data with different levels of noise or corruption as they learn to extract robust features. This is beneficial in various applications where data quality is compromised, such as image denoising or signal processing.

### Objective Function of DAE

The objective of DAE is to minimize the difference between the original input (clean input without the notice) and the reconstructed output. This is quantified using a reconstruction loss function. Two types of loss function are generally used depending on the type of input data.

#### Mean Squared Error (MSE):

If we have input image data in the form of floating pixel values i.e. values between (0 to 1) or (0 to 255) we use mse

MSE(x,y)=1mΣi=1m(xi−yi)2MSE(x,y) = \\frac{1}{m}\\Sigma\_{i=1}^m(x\_i-y\_i)^2MSE(x,y)=m1​Σi=1m​(xi​−yi​)2

Here,

- each of xi is the pixel value of input data
- yi is the pixel value of reconstructed data
  - yi = D(E(xi\*noise) )
  - Where E represents encoder and D represents decoder
- this is summed over all the training set

#### Binary Cross-Entropy (log-loss):

If we have input image data in the form of bits pixel values i.e. values will be either 0 or 1 only then we can use binary cross entrop loss for each pixel value

LL(x,y)=−1mΣi=1m(xiln(yi)+(1−xi)ln(1−yi))LL(x,y) = - \\frac {1}{m}\\Sigma\_{i=1}^m( x\_i ln(y\_i) + (1-x\_i)ln(1-y\_i))LL(x,y)=−m1​Σi=1m​(xi​ln(yi​)+(1−xi​)ln(1−yi​))

Here

- each of xi is the pixel value of input data with value being only 0 or 1
- yi is the pixel value of reconstructed data.
  - yi = D(E(xi\*noise))
- Where E represents encoder and D represents decoder
- this is summed over all the training set

### Training Process of DAE

The training of DAE consists of below steps:

- Initialze ender and decoer with random weights
- Noise is intentionally added to the input data.
- Feedforward the input data through encoder and decoder to get the reconstructed image
- Calculate the reconstruction loss as defined in our objective function w
- Do backprogoagation and update weights.The goal during training is to minimize the reconstruction loss.

The training is typically done through optimization algorithms like stochastic gradient descent (SGD) or its variants.

### Applications of DAE

- Image Denoising: DAEs are widely employed for cleaning and enhancing images by removing noise.
- Audio Denoising: DAEs can be applied to denoise audio signals, making them valuable in speech-enhancement tasks.
- Sensor Data Processing: DAEs are valuable in processing sensor data, removing noise, and extracting relevant information from sensor readings.
- Data Compression: Autoencoders, including DAEs, can be utilized for data compression by learning compact representations of input data.
- Feature Learning: DAEs are effective in unsupervised feature learning, capturing relevant features in the data without explicit labels.

![DAE](https://media.geeksforgeeks.org/wp-content/uploads/20240107093422/DAE.jpeg)DAE architecture

### Implementation of DAE

Let us implement DAE in PyTorch for MNIST dataset.

#### 1\. Import Libraries

- torch. utils.data provides tools for working with datasets and data loaders in [PyTorch](https://www.geeksforgeeks.org/getting-started-with-pytorch/).
- torch-vision is a PyTorch library specifically designed for [computer vision](https://www.geeksforgeeks.org/computer-vision/) tasks. datasets contain popular datasets (like MNIST, CIFAR-10, etc.), and transforms provide image transformations and preprocessing functions.
- nn provides building blocks for constructing [neural network](https://www.geeksforgeeks.org/neural-networks-a-beginners-guide/) architectures, and optim includes optimization algorithms (like [SGD](https://www.geeksforgeeks.org/ml-stochastic-gradient-descent-sgd/), [Adam](https://www.geeksforgeeks.org/intuition-of-adam-optimizer/), etc.) for training neural networks.
- If a GPU is available, it sets the device variable to 'cuda'; otherwise, it sets it to 'CPU'

Python`
import torch.utils.data
from torchvision import datasets, transforms
import numpy as np
import pandas as pd
from torch import nn, optim
device = 'cuda' if torch.cuda.is_available() else 'cpu'
`

#### 2\. Define Dataloader

- DataLoader is used to efficiently load and iterate over batches of data during training.
- We define a transform object.
  - transforms.ToTensor(): Converts the image data into PyTorch tensors.
  - transforms.Normalize(0, 1): Normalizes the tensor values to have a mean of 0 and a standard deviation of 1.
- We then load the MNIST dataset.
  - The root parameter specifies the directory where the dataset will be stored.
  - train=True indicates that it is the training set , train = False indicates that it is the test set
  - download=True ensures that the dataset is downloaded if not already available
  - transform=transform applies the defined transform
- We have a DataLoader (train\_loader and test\_loader) ready to be used in your training loop.

Python`
from torch.utils.data import DataLoader
transform = transforms.Compose(
    [transforms.ToTensor(), transforms.Normalize(0, 1)])
# Load the MNIST dataset
mnist_dataset_train = datasets.MNIST(
    root='./data', train=True, download=True, transform=transform)
# Load the test MNIST dataset
mnist_dataset_test = datasets.MNIST(
    root='./data', train=False, download=True, transform=transform)
batch_size = 128
train_loader = torch.utils.data.DataLoader(
    mnist_dataset_train, batch_size=batch_size, shuffle=True)
test_loader = torch.utils.data.DataLoader(
    mnist_dataset_test, batch_size=5, shuffle=False)
`

#### 3\. Define our Model

- We have three fully connected layers in the encoder (fc1, fc2, fc3) and three in the decoder (fc4, fc5, fc6).
- The [ReLU](https://www.geeksforgeeks.org/activation-functions/) activation function is used for the hidden layers, and the [Sigmoid activation](https://www.geeksforgeeks.org/types-of-activation-function-in-ann/) is used for the output layer to squash the values between 0 and 1.
- Encode Method (encode):
  - The encoding method takes an input tensor x and then passes it through the encoder layers.
  - The input is noisy input .
  - The noisy input is then passed through the encoder layers (fc1, fc2, fc3), and the result is returned.
- Decode Method (decode):
  - The decode method takes the encoded representation (z) and passes it through the decoder layers (fc4, fc5, fc6).
  - The output is passed through the Sigmoid activation function to ensure values are between 0 and 1.
- Forward Method (forward):
  - The forward method defines the forward pass of the autoencoder.
  - It calls the encode method with the input tensor x, obtains the encoded representation (q), and then passes it through the decode method.
  - The final reconstructed output is returned.

Python`
class DAE(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(784, 512)
        self.fc2 = nn.Linear(512, 256)
        self.fc3 = nn.Linear(256, 128)
        self.fc4 = nn.Linear(128, 256)
        self.fc5 = nn.Linear(256, 512)
        self.fc6 = nn.Linear(512, 784)
        self.relu = nn.ReLU()
        self.sigmoid = nn.Sigmoid()
    def encode(self, x):
        h1 = self.relu(self.fc1(x))
        h2 = self.relu(self.fc2(h1))
        return self.relu(self.fc3(h2))
    def decode(self, z):
        h4 = self.relu(self.fc4(z))
        h5 = self.relu(self.fc5(h4))
        return self.sigmoid(self.fc6(h5))
    def forward(self, x):
        q = self.encode(x.view(-1, 784))
        return self.decode(q)
`

#### 4\. Define our train function

We define a train function that:

- It takes an epoch number, the model, a data loader (train\_loader), an optimizer, and an optional boolean cuda indicating whether to use CUDA (GPU) for training.
- Initializes a variable train\_loss to keep track of the cumulative loss during training.
- Iterates over batches of data from the train\_loader. Each batch consists of input data (data) and their corresponding labels (though labels are not used in this loop).
- Clears the gradients in the optimizer before the backward pass.
- Passes the input data through the autoencoder model to obtain the reconstructed output.
  - Here note that we are passing a noisy input to the encoder by adding gaussian noise
- Computes the loss between the reconstructed output (recon\_batch) and the original input (data) output.
- Updates the model parameters using the [optimizer.](https://www.geeksforgeeks.org/optimizers-in-tensorflow/)
- Prints the training progress every 100 batches.

Python`
def train(epoch, model, train_loader, optimizer,  cuda=True):
    model.train()
    train_loss = 0
    for batch_idx, (data, _) in enumerate(train_loader):
        data.to(device)
        optimizer.zero_grad()
        data_noise = torch.randn(data.shape).to(device)
        data_noise = data + data_noise
        recon_batch = model(data_noise.to(device))
        loss = criterion(recon_batch, data.view(data.size(0), -1).to(device))
        loss.backward()
        train_loss += loss.item() * len(data)
        optimizer.step()
        if batch_idx % 100 == 0:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(epoch, batch_idx * len(data), len(train_loader.dataset),
                                                                           100. * batch_idx /
                                                                           len(train_loader),
                                                                           loss.item()))
    print('====&gt; Epoch: {} Average loss: {:.4f}'.format(
        epoch, train_loss / len(train_loader.dataset)))
`

#### 5\. Define model, optimizer, and loss function

- We create an instance of your Denoising Autoencoder (DAE) model and move it to the specified device.
- We define Adam optimizer to optimize the model parameters. The learning rate is set to 1e-2 (0.01)
- We define [Mean Squared Error (MSE)](https://www.geeksforgeeks.org/python-mean-squared-error/) loss as the criterion. This loss measures the average squared difference between the input and target.

Python`
epochs = 10
model = DAE().to(device)
optimizer = optim.Adam(model.parameters(), lr=1e-2)
criterion = nn.MSELoss()
`

#### 6\. Train our model

- We train our model for the specified epochs.

Python`
for epoch in range(1, epochs + 1):
    train(epoch, model, train_loader, optimizer, True)
`

**Output:**

```
Train Epoch: 1 [0/60000 (0%)]    Loss: 0.232055
Train Epoch: 1 [12800/60000 (21%)]    Loss: 0.055512
Train Epoch: 1 [25600/60000 (43%)]    Loss: 0.050033
Train Epoch: 1 [38400/60000 (64%)]    Loss: 0.041521
Train Epoch: 1 [51200/60000 (85%)]    Loss: 0.043305
====> Epoch: 1 Average loss: 0.0509
Train Epoch: 2 [0/60000 (0%)]    Loss: 0.041658
Train Epoch: 2 [12800/60000 (21%)]    Loss: 0.040901
Train Epoch: 2 [25600/60000 (43%)]    Loss: 0.040894
Train Epoch: 2 [38400/60000 (64%)]    Loss: 0.039513
Train Epoch: 2 [51200/60000 (85%)]    Loss: 0.041100
====> Epoch: 2 Average loss: 0.0407
Train Epoch: 3 [0/60000 (0%)]    Loss: 0.041685
Train Epoch: 3 [12800/60000 (21%)]    Loss: 0.039040
Train Epoch: 3 [25600/60000 (43%)]    Loss: 0.038953
Train Epoch: 3 [38400/60000 (64%)]    Loss: 0.038851
Train Epoch: 3 [51200/60000 (85%)]    Loss: 0.040141
```

#### 7\. Performance of the model

- We use our test model to evaluate the performance of model
- Let us take 5 data samples from our test data loader add noise to it and pass it through our model.
- We will then view the output of our model and compare it with our clean data.

Python`
import matplotlib.pyplot as plt
for batch_idx, (data, labels) in enumerate(test_loader):
    data.to(device)
    optimizer.zero_grad()
    data_noise = torch.randn(data.shape).to(device)
    data_noise = data + data_noise
    recon_batch = model(data_noise.to(device))
    break
plt.figure(figsize=(20, 12))
for i in range(5):
    print(f&quot; Image {i} with label {labels[i]}              &quot;, end=&quot;&quot;)
    plt.subplot(3, 5, 1+i)
    plt.imshow(data_noise[i, :, :, :].view(
        28, 28).detach().numpy(), cmap='binary')
    plt.subplot(3, 5, 6+i)
    plt.imshow(recon_batch[i, :].view(28, 28).detach().numpy(), cmap='binary')
    plt.axis('off')
    plt.subplot(3, 5, 11+i)
    plt.imshow(data[i, :, :, :].view(28, 28).detach().numpy(), cmap='binary')
    plt.axis('off')
plt.show()
`

**Output:**

![file](https://media.geeksforgeeks.org/wp-content/uploads/20240109162658/file.jpg)

- The first row is the corrupted image
- The second row is the reconstructed image
- The last row is the actual image before addition of noise

We see that the model is able to reconstruct our original image quite well compared to our actual image with only 10 epochs of training.

### Conclusion

In this article, we saw a variation of auto encoders namely denoising auto encoders, its application and its implementation in python using MNIST dataset and PyTorch framework.

Comment


More info

[Advertise with us](https://www.geeksforgeeks.org/about/contact-us/?listicles)

[Next Article](https://www.geeksforgeeks.org/monte-carlo-policy-evaluation/)

[Monte Carlo Policy Evaluation](https://www.geeksforgeeks.org/monte-carlo-policy-evaluation/)

[R](https://www.geeksforgeeks.org/user/rahulsm27/)

[rahulsm27](https://www.geeksforgeeks.org/user/rahulsm27/)

Follow

2

Improve

Article Tags :

- [Machine Learning](https://www.geeksforgeeks.org/category/ai-ml-ds/machine-learning/)
- [AI-ML-DS](https://www.geeksforgeeks.org/category/ai-ml-ds/)
- [Deep-Learning](https://www.geeksforgeeks.org/tag/deep-learning/)

Practice Tags :

- [Machine Learning](https://www.geeksforgeeks.org/explore?category=Machine%20Learning)

### Similar Reads

[Autoencoders -Machine Learning\\
\\
\\
An autoencoder is a type of artificial neural network that learns to represent data in a compressed form and then reconstructs it as closely as possible to the original input. Autoencoders consists of two components: Encoder: This compresses the input into a compact representation and capture the mo\\
\\
9 min read](https://www.geeksforgeeks.org/auto-encoders/)
[Masked Autoencoders in Deep Learning\\
\\
\\
Masked autoencoders are neural network models designed to reconstruct input data from partially masked or corrupted versions, helping the model learn robust feature representations. They are significant in deep learning for tasks such as data denoising, anomaly detection, and improving model general\\
\\
12 min read](https://www.geeksforgeeks.org/masked-autoencoders-in-deep-learning/)
[Sparse Autoencoders in Deep Learning\\
\\
\\
Sparse autoencoders are aÂ specificÂ formÂ of autoencoderÂ that'sÂ beenÂ trainedÂ forÂ feature learning and dimensionality reduction.Â AsÂ opposedÂ to regularÂ autoencoders,Â whichÂ areÂ trainedÂ toÂ reconstructÂ the input data in the output, sparse autoencodersÂ addÂ a sparsityÂ penaltyÂ thatÂ encouragesÂ the hidden layer\\
\\
5 min read](https://www.geeksforgeeks.org/sparse-autoencoders-in-deep-learning/)
[Curse of Dimensionality in Machine Learning\\
\\
\\
Curse of Dimensionality in Machine Learning arises when working with high-dimensional data, leading to increased computational complexity, overfitting, and spurious correlations. Techniques like dimensionality reduction, feature selection, and careful model design are essential for mitigating its ef\\
\\
5 min read](https://www.geeksforgeeks.org/curse-of-dimensionality-in-machine-learning/)
[What is AutoML in Machine Learning?\\
\\
\\
Automated Machine Learning (automl) addresses the challenge of democratizing machine learning by automating the complex model development process. With applications in various sectors, AutoML aims to make machine learning accessible to those lacking expertise. The article highlights the growing sign\\
\\
13 min read](https://www.geeksforgeeks.org/what-is-automl-in-machine-learning/)
[What are Embedding in Machine Learning?\\
\\
\\
In recent years, embeddings have emerged as a core idea in machine learning, revolutionizing the way we represent and understand data. In this article, we delve into the world of embeddings, exploring their importance, applications, and the underlying techniques used to generate them. Table of Conte\\
\\
15+ min read](https://www.geeksforgeeks.org/what-are-embeddings-in-machine-learning/)
[Managing High-Dimensional Data in Machine Learning\\
\\
\\
High-dimensional input spaces are a common challenge in machine learning, particularly in fields such as genomics, image processing, and natural language processing. These datasets contain a vast number of features, making them complex and difficult to manage. The "curse of dimensionality," a term c\\
\\
6 min read](https://www.geeksforgeeks.org/managing-high-dimensional-data-in-machine-learning/)
[What is Data Acquisition in Machine Learning?\\
\\
\\
Data acquisition, or DAQ, is the cornerstone of machine learning. It is essential for obtaining high-quality data for model training and optimizing performance. Data-centric techniques are becoming more and more important across a wide range of industries, and DAQ is now a vital tool for improving p\\
\\
12 min read](https://www.geeksforgeeks.org/what-is-data-acquisition-in-machine-learning/)
[Diffusion Models in Machine Learning\\
\\
\\
A diffusion model in machine learning is a probabilistic framework that models the spread and transformation of data over time to capture complex patterns and dependencies. In this article, we are going to explore the fundamentals of diffusion models and implement diffusion models to generate images\\
\\
9 min read](https://www.geeksforgeeks.org/diffusion-models-in-machine-learning/)
[Disentanglement in Beta Variational Autoencoders\\
\\
\\
Beta Variational Autoencoders was proposed by researchers at Deepmind in 2017. It was accepted in the International Conference on Learning Representations (ICLR) 2017. Before learning Beta- variational autoencoder, please check out this article for variational autoencoder. If in variational autoenco\\
\\
4 min read](https://www.geeksforgeeks.org/disentanglement-in-beta-variational-autoencoders/)

Like2

We use cookies to ensure you have the best browsing experience on our website. By using our site, you
acknowledge that you have read and understood our
[Cookie Policy](https://www.geeksforgeeks.org/cookie-policy/) &
[Privacy Policy](https://www.geeksforgeeks.org/privacy-policy/)
Got It !


![Lightbox](https://www.geeksforgeeks.org/denoising-autoencoders-in-machine-learning/)

Improvement

Suggest changes

Suggest Changes

Help us improve. Share your suggestions to enhance the article. Contribute your expertise and make a difference in the GeeksforGeeks portal.

![geeksforgeeks-suggest-icon](https://media.geeksforgeeks.org/auth-dashboard-uploads/suggestChangeIcon.png)

Create Improvement

Enhance the article with your expertise. Contribute to the GeeksforGeeks community and help create better learning resources for all.

![geeksforgeeks-improvement-icon](https://media.geeksforgeeks.org/auth-dashboard-uploads/createImprovementIcon.png)

Suggest Changes

min 4 words, max Words Limit:1000

## Thank You!

Your suggestions are valuable to us.

## What kind of Experience do you want to share?

[Interview Experiences](https://write.geeksforgeeks.org/posts-new?cid=e8fc46fe-75e7-4a4b-be3c-0c862d655ed0) [Admission Experiences](https://write.geeksforgeeks.org/posts-new?cid=82536bdb-84e6-4661-87c3-e77c3ac04ede) [Career Journeys](https://write.geeksforgeeks.org/posts-new?cid=5219b0b2-7671-40a0-9bda-503e28a61c31) [Work Experiences](https://write.geeksforgeeks.org/posts-new?cid=22ae3354-15b6-4dd4-a5b4-5c7a105b8a8f) [Campus Experiences](https://write.geeksforgeeks.org/posts-new?cid=c5e1ac90-9490-440a-a5fa-6180c87ab8ae) [Competitive Exam Experiences](https://write.geeksforgeeks.org/posts-new?cid=5ebb8fe9-b980-4891-af07-f2d62a9735f2)

[iframe](https://td.doubleclick.net/td/ga/rul?tid=G-DWCCJLKX3X&gacid=2095684707.1745057318&gtm=45je54h0h2v884918195za200&dma=0&gcd=13l3l3l3l1l1&npa=0&pscdl=noapi&aip=1&fledge=1&frm=0&tag_exp=102803279~102813109~102887800~102926062~103027016~103051953~103055465~103077950~103106314~103106316&z=2015728449)[iframe](https://td.doubleclick.net/td/rul/796001856?random=1745057318688&cv=11&fst=1745057318688&fmt=3&bg=ffffff&guid=ON&async=1&gtm=45be54h0h2v877914216za200zb884918195&gcd=13l3l3R3l5l1&dma=0&tag_exp=102803279~102813109~102887800~102926062~103027016~103051953~103055465~103077950~103106314~103106316~103116025&ptag_exp=102803279~102813109~102887800~102926062~103027016~103051953~103055465~103077950~103106314~103106316&u_w=1280&u_h=1024&url=https%3A%2F%2Fwww.geeksforgeeks.org%2Fdenoising-autoencoders-in-machine-learning%2F&hn=www.googleadservices.com&frm=0&tiba=Denoising%20AutoEncoders%20In%20Machine%20Learning%20%7C%20GeeksforGeeks&npa=0&pscdl=noapi&auid=2009439540.1745057319&uaa=x86&uab=64&uafvl=Google%2520Chrome%3B135.0.7049.95%7CNot-A.Brand%3B8.0.0.0%7CChromium%3B135.0.7049.95&uamb=0&uam=&uap=Linux%20x86_64&uapv=6.6.72&uaw=0&fledge=1&data=event%3Dgtag.config)

Login Modal \| GeeksforGeeks

# Log in

New user ?Register Now

Continue with Google

or

Username or Email

Password

Remember me

Forgot Password

Sign In

By creating this account, you agree to our [Privacy Policy](https://www.geeksforgeeks.org/privacy-policy/) & [Cookie Policy.](https://www.geeksforgeeks.org/legal/privacy-policy/#:~:text=the%20appropriate%20measures.-,COOKIE%20POLICY,-A%20cookie%20is)

# Create Account

Already have an account ?Log in

Continue with Google

or

Username or Email

Password

Institution / Organization

```

```

Sign Up

\*Please enter your email address or userHandle.

Back to Login

Reset Password

[iframe](about:blank)[iframe](about:blank)[iframe](about:blank)[iframe](about:blank)[iframe](about:blank)[iframe](https://securepubads.g.doubleclick.net/static/topics/topics_frame.html)